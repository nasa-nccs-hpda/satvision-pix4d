PIPELINE: 'satvision_pix4d_satmae_pretrain'
DATAMODULE: 'abi_temporal'

OUTPUT: '/raid/jacaraba/models'

MODEL:
  TYPE: satmae
  NAME: satmae_satvision_pix4d_pretrain-dev
  DROP_PATH_RATE: 0.0                 # ↓ turn off regularization
  MAE_VIT:
    PATCH_SIZE: 16
    IN_CHANS: 16
    EMBED_DIM: 1024
    DEPTHS: 24
    NUM_HEADS: 16
    MLP_RATIO: 4.0
    DECODER_EMBED_DIM: 512
    DECODER_DEPTH: 8
    DECODER_NUM_HEADS: 16
    SAME_MASK: False

DATA:
  DATAMODULE: True
  BATCH_SIZE: 1                        # ensure >1 batches/epoch with 10 tiles
  LENGTH: 200                          # if your datamodule uses this for steps/epoch
  SHUFFLE: True
  PIN_MEMORY: True
  NUM_WORKERS: 4
  PERSISTENT_WORKERS: True
  TRAIN_DATA_PATHS: ["/raid/jacaraba/convection-small"]
  VAL_DATA_PATHS:   ["/raid/jacaraba/convection-small"]
  IMG_SIZE: 512
  MASK_PATCH_SIZE: 16                  # match model patch size (or drop this field)
  MASK_RATIO: 0.25

TRAIN:
  ACCELERATOR: 'gpu'
  STRATEGY: 'auto'                     # keep it simple; avoid deepspeed here
  USE_CHECKPOINT: True
  EPOCHS: 60                           # enough to see clear overfit
  WARMUP_EPOCHS: 2                     # with tiny steps, keep warmup short
  BASE_LR: 1e-3                        # push learning faster for overfit
  MIN_LR: 1e-6                         # allow cosine to decay
  WARMUP_LR: 1e-6
  WEIGHT_DECAY: 0.0                    # ↓ turn off regularization
  LR_SCHEDULER:
    NAME: 'cosine'                     # note: your module overrides this anyway

MLFLOW:
  URI: 'file:///raid/jacaraba/mlruns/satvision-pix4d'

DEEPSPEED:
  STAGE: 0                             # effectively off; ignored if STRATEGY='auto'

PRECISION: '32-true'                   # avoid bf16/amp for debugging

PRINT_FREQ: 1
SAVE_FREQ: 10
TAG: satmae_satvision_pix4d_pretrain-dev
