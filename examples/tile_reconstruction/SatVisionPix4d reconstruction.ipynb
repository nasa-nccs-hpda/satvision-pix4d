{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07079c64-af33-4833-b8ea-9238ab8edaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sys.path.append('/explore/nobackup/people/jacaraba/development/satvision-pix4d')\n",
    "\n",
    "from satvision_pix4d.configs.config import _C, _update_config_from_file\n",
    "from satvision_pix4d.pipelines import PIPELINES, get_available_pipelines\n",
    "from satvision_pix4d.datasets.abi_temporal_benchmark_dataset import ABITemporalBenchmarkDataset\n",
    "from satvision_pix4d.datasets.abi_temporal_dataset import ABITemporalDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5036c029-0a45-4001-87b0-bcbf383f91a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whether to save files to a PDF, and where to save them \n",
    "save_to_pdf = False\n",
    "pdf_path = \"chip_plot.pdf\"\n",
    "\n",
    "# RGB indices for ABI data (16 channels instead of 14)\n",
    "rgb_index = [0, 2, 1]  # Adjust based on your ABI channels\n",
    "\n",
    "# Use your local model paths instead of downloading\n",
    "model_filename = '/explore/nobackup/projects/pix4dcloud/jacaraba/model_development/satmae/' + \\\n",
    "    'satmae_satvision_pix4d_pretrain-dev/satmae_satvision_pix4d_pretrain-dev/epoch-epoch=40.ckpt/checkpoint/mp_rank_00_model_states.pt'\n",
    "\n",
    "config_filename = '/explore/nobackup/people/jacaraba/development/satvision-pix4d/tests/configs/test_satmae_dev.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb22d97d-0564-4e26-b357-8f16e068f4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config = _C.clone()\n",
    "_update_config_from_file(config, config_filename)\n",
    "print(\"Loaded configuration file.\")\n",
    "\n",
    "# Update config with your paths\n",
    "config.defrost()\n",
    "config.MODEL.PRETRAINED = model_filename\n",
    "config.OUTPUT = '.'\n",
    "config.freeze()\n",
    "print(\"Updated configuration file.\")\n",
    "\n",
    "# Get pipeline and load model\n",
    "available_pipelines = get_available_pipelines()\n",
    "print(\"Available pipelines:\", available_pipelines)\n",
    "\n",
    "pipeline = PIPELINES[config.PIPELINE]\n",
    "print(f'Using {pipeline}')\n",
    "\n",
    "ptlPipeline = pipeline(config)\n",
    "\n",
    "# Load model\n",
    "print(f'Attempting to load checkpoint from {config.MODEL.PRETRAINED}')\n",
    "model = ptlPipeline.load_checkpoint(config.MODEL.PRETRAINED, config)\n",
    "print('Successfully applied checkpoint')\n",
    "\n",
    "model.cpu()\n",
    "model.eval()\n",
    "print('Successfully moved to CPU and eval mode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab43139-25dc-44c2-aef2-9d23f617c0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "data_dir = '/explore/nobackup/projects/pix4dcloud/jacaraba/tiles_pix4d/3-tiles/convection/20200101'\n",
    "\n",
    "all_zarr_files = [f for f in os.listdir(data_dir) if f.endswith('.zarr')]\n",
    "print(f\"Found {len(all_zarr_files)} zarr files in directory\")\n",
    "\n",
    "train_ds = ABITemporalDataset(\n",
    "    data_paths=[data_dir], \n",
    "    img_size=512,\n",
    "    in_chans=16,\n",
    "    data_var='__xarray_dataarray_variable__'\n",
    ")\n",
    "\n",
    "print(f\"Dataset contains {len(train_ds)} samples from all files\")\n",
    "\n",
    "# Process multiple samples from the dataset\n",
    "num_samples = min(5, len(train_ds))  \n",
    "print(f\"Processing {num_samples} samples from multiple files...\")\n",
    "\n",
    "all_inputs = []\n",
    "all_outputs = []\n",
    "all_masks = []\n",
    "all_losses = []\n",
    "\n",
    "print(f\"\\nProcessing samples for reconstruction...\")\n",
    "for sample_idx in tqdm(range(num_samples)):\n",
    "    try:\n",
    "        imgs, ts = train_ds[sample_idx]\n",
    "        \n",
    "        # NO NORMALIZATION - use raw data as-is\n",
    "        # Just ensure it's a tensor and add batch dimension\n",
    "        imgs_batch = imgs.unsqueeze(0).cpu()\n",
    "        if isinstance(ts, np.ndarray):\n",
    "            ts = torch.from_numpy(ts).float()\n",
    "        ts_batch = ts.unsqueeze(0).cpu()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            loss, pred, mask = model(imgs_batch, ts_batch)\n",
    "            \n",
    "            B, T, C, H, W = imgs_batch.shape\n",
    "            pred_imgs = model.model.unpatchify(pred, T, H, W)\n",
    "            pred_imgs = torch.clamp(pred_imgs, 0, 1)\n",
    "        \n",
    "        # Store results\n",
    "        all_inputs.append(imgs_batch.cpu().squeeze(0))\n",
    "        all_outputs.append(pred_imgs.cpu().squeeze(0))\n",
    "        all_masks.append(mask.cpu().squeeze(0))\n",
    "        all_losses.append(loss.cpu().item())\n",
    "        \n",
    "        if sample_idx % 10 == 0:  # Print progress every 10 samples\n",
    "            print(f\"Sample {sample_idx}: Loss = {loss.item():.2f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing sample {sample_idx}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n=== FINAL RESULTS ===\")\n",
    "print(f\"Total samples processed: {len(all_inputs)}\")\n",
    "print(f\"Average loss across all samples: {np.mean(all_losses):.4f}\")\n",
    "\n",
    "# Update global variables for compatibility with existing code\n",
    "inputs = all_inputs\n",
    "outputs = all_outputs\n",
    "masks = all_masks\n",
    "losses = all_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d2eaac-462a-4306-ab68-91c73261ef47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import structural_similarity as ssim\n",
    "import numpy as np\n",
    "\n",
    "def calculate_mse(original, reconstructed):\n",
    "    return torch.mean((original - reconstructed) ** 2).item()\n",
    "\n",
    "def calculate_psnr(mse, data_range=1.0):\n",
    "    if mse == 0:\n",
    "        return float('inf')\n",
    "    return 20 * np.log10(data_range / np.sqrt(mse))\n",
    "\n",
    "def calculate_mae(original, reconstructed):\n",
    "    return torch.mean(torch.abs(original - reconstructed)).item()\n",
    "\n",
    "def calculate_ssim_fast(original, reconstructed, sample_channels=4):\n",
    "    \"\"\"Fast SSIM - only sample a few channels to speed up calculation\"\"\"\n",
    "    if isinstance(original, torch.Tensor):\n",
    "        original = original.numpy()\n",
    "    if isinstance(reconstructed, torch.Tensor):\n",
    "        reconstructed = reconstructed.numpy()\n",
    "    \n",
    "    ssim_scores = []\n",
    "    \n",
    "    #sample\n",
    "    total_channels = original.shape[0] * original.shape[1]  # T * C\n",
    "    channel_indices = np.linspace(0, total_channels-1, sample_channels, dtype=int)\n",
    "    \n",
    "    for idx in channel_indices:\n",
    "        t_idx = idx // original.shape[1]  # timestep\n",
    "        c_idx = idx % original.shape[1]   # channel\n",
    "        \n",
    "        if t_idx < original.shape[0] and c_idx < original.shape[1]:\n",
    "            try:\n",
    "                score = ssim(original[t_idx, c_idx], reconstructed[t_idx, c_idx], \n",
    "                           data_range=1.0)\n",
    "                ssim_scores.append(score)\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    return np.mean(ssim_scores) if ssim_scores else 0.0\n",
    "\n",
    "\n",
    "print(\"Calculating comprehensive metrics for large dataset...\")\n",
    "print(f\"Processing {len(inputs)} samples...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "\n",
    "metrics_results = []\n",
    "batch_size = 20  \n",
    "\n",
    "# Process in batches\n",
    "for batch_start in range(0, len(inputs), batch_size):\n",
    "    batch_end = min(batch_start + batch_size, len(inputs))\n",
    "    batch_indices = range(batch_start, batch_end)\n",
    "    \n",
    "    print(f\"Processing batch {batch_start//batch_size + 1}/{(len(inputs)-1)//batch_size + 1} \"\n",
    "          f\"(samples {batch_start}-{batch_end-1})\")\n",
    "    \n",
    "    batch_metrics = []\n",
    "    \n",
    "    for i in batch_indices:\n",
    "        original = inputs[i]\n",
    "        reconstructed = outputs[i]\n",
    "        \n",
    "        mse = calculate_mse(original, reconstructed)\n",
    "        mae = calculate_mae(original, reconstructed)\n",
    "        psnr = calculate_psnr(mse)\n",
    "        \n",
    "        ssim_score = calculate_ssim_fast(original, reconstructed, sample_channels=4)\n",
    "        \n",
    "        rgb_indices = [1, 0, 2]\n",
    "        rgb_metrics = {}\n",
    "        \n",
    "        for band_name, band_idx in zip(['red', 'green', 'blue'], rgb_indices):\n",
    "            if band_idx < original.shape[1]:\n",
    "                \n",
    "                orig_band = original[:, band_idx, :, :].mean(dim=0)\n",
    "                recon_band = reconstructed[:, band_idx, :, :].mean(dim=0)\n",
    "                \n",
    "                band_mse = calculate_mse(orig_band, recon_band)\n",
    "                band_psnr = calculate_psnr(band_mse)\n",
    "                \n",
    "                rgb_metrics[band_name] = {'mse': band_mse, 'psnr': band_psnr}\n",
    "        \n",
    "        sample_metrics = {\n",
    "            'sample_idx': i,\n",
    "            'loss': losses[i],\n",
    "            'mse': mse,\n",
    "            'mae': mae,\n",
    "            'psnr': psnr,\n",
    "            'ssim': ssim_score,\n",
    "            'rgb_metrics': rgb_metrics\n",
    "        }\n",
    "        \n",
    "        batch_metrics.append(sample_metrics)\n",
    "    \n",
    "    metrics_results.extend(batch_metrics)\n",
    "    \n",
    "    # Print batch summary\n",
    "    batch_mse = [m['mse'] for m in batch_metrics]\n",
    "    batch_psnr = [m['psnr'] for m in batch_metrics]\n",
    "    print(f\"  Batch avg MSE: {np.mean(batch_mse):.6f}, avg PSNR: {np.mean(batch_psnr):.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LARGE DATASET METRICS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Overall statistics\n",
    "all_mse = [m['mse'] for m in metrics_results]\n",
    "all_mae = [m['mae'] for m in metrics_results]\n",
    "all_psnr = [m['psnr'] for m in metrics_results]\n",
    "all_ssim = [m['ssim'] for m in metrics_results]\n",
    "all_loss = [m['loss'] for m in metrics_results]\n",
    "\n",
    "print(f\"Dataset Statistics (n={len(metrics_results)} samples):\")\n",
    "print(f\"  MSE:  {np.mean(all_mse):.6f} ± {np.std(all_mse):.6f}\")\n",
    "print(f\"  MAE:  {np.mean(all_mae):.6f} ± {np.std(all_mae):.6f}\")\n",
    "print(f\"  PSNR: {np.mean(all_psnr):.2f} ± {np.std(all_psnr):.2f} dB\")\n",
    "print(f\"  SSIM: {np.mean(all_ssim):.4f} ± {np.std(all_ssim):.4f}\")\n",
    "print(f\"  Loss: {np.mean(all_loss):.2f} ± {np.std(all_loss):.2f}\")\n",
    "\n",
    "\n",
    "print(f\"\\nPerformance Distribution Analysis:\")\n",
    "percentiles = [10, 25, 50, 75, 90]\n",
    "mse_percentiles = np.percentile(all_mse, percentiles)\n",
    "psnr_percentiles = np.percentile(all_psnr, percentiles)\n",
    "\n",
    "print(f\"MSE Percentiles:\")\n",
    "for p, val in zip(percentiles, mse_percentiles):\n",
    "    print(f\"  {p}th: {val:.6f}\")\n",
    "\n",
    "print(f\"PSNR Percentiles:\")\n",
    "for p, val in zip(percentiles, psnr_percentiles):\n",
    "    print(f\"  {p}th: {val:.2f} dB\")\n",
    "\n",
    "\n",
    "mse_threshold_good = np.percentile(all_mse, 33)  # Bottom third\n",
    "mse_threshold_bad = np.percentile(all_mse, 67)   # Top third\n",
    "\n",
    "good_samples = [i for i, m in enumerate(all_mse) if m <= mse_threshold_good]\n",
    "medium_samples = [i for i, m in enumerate(all_mse) if mse_threshold_good < m <= mse_threshold_bad]\n",
    "bad_samples = [i for i, m in enumerate(all_mse) if m > mse_threshold_bad]\n",
    "\n",
    "print(f\"\\nPerformance Categories:\")\n",
    "print(f\"  Good samples (bottom 33%): {len(good_samples)} samples\")\n",
    "print(f\"    Avg MSE: {np.mean([all_mse[i] for i in good_samples]):.6f}\")\n",
    "print(f\"    Avg PSNR: {np.mean([all_psnr[i] for i in good_samples]):.2f} dB\")\n",
    "\n",
    "print(f\"  Medium samples (middle 33%): {len(medium_samples)} samples\")\n",
    "print(f\"    Avg MSE: {np.mean([all_mse[i] for i in medium_samples]):.6f}\")\n",
    "print(f\"    Avg PSNR: {np.mean([all_psnr[i] for i in medium_samples]):.2f} dB\")\n",
    "\n",
    "print(f\"  Bad samples (top 33%): {len(bad_samples)} samples\")\n",
    "print(f\"    Avg MSE: {np.mean([all_mse[i] for i in bad_samples]):.6f}\")\n",
    "print(f\"    Avg PSNR: {np.mean([all_psnr[i] for i in bad_samples]):.2f} dB\")\n",
    "\n",
    "\n",
    "print(f\"\\nRGB Band Performance Summary:\")\n",
    "for band_name in ['red', 'green', 'blue']:\n",
    "    band_mse = [m['rgb_metrics'][band_name]['mse'] for m in metrics_results \n",
    "                if band_name in m['rgb_metrics']]\n",
    "    band_psnr = [m['rgb_metrics'][band_name]['psnr'] for m in metrics_results \n",
    "                 if band_name in m['rgb_metrics']]\n",
    "    \n",
    "    if band_mse:\n",
    "        print(f\"  {band_name.capitalize()}: MSE={np.mean(band_mse):.6f}, PSNR={np.mean(band_psnr):.2f}dB\")\n",
    "\n",
    "# Best and worst samples\n",
    "best_idx = np.argmin(all_mse)\n",
    "worst_idx = np.argmax(all_mse)\n",
    "\n",
    "print(f\"\\nExtreme Samples:\")\n",
    "print(f\"  Best sample #{best_idx}: MSE={all_mse[best_idx]:.6f}, PSNR={all_psnr[best_idx]:.2f}dB\")\n",
    "print(f\"  Worst sample #{worst_idx}: MSE={all_mse[worst_idx]:.6f}, PSNR={all_psnr[worst_idx]:.2f}dB\")\n",
    "print(f\"  Performance range: {all_mse[worst_idx]/all_mse[best_idx]:.1f}x difference\")\n",
    "\n",
    "# Save results summary\n",
    "performance_categories = {\n",
    "    'good': good_samples,\n",
    "    'medium': medium_samples, \n",
    "    'bad': bad_samples\n",
    "}\n",
    "\n",
    "print(f\"\\nMetrics calculation complete!\")\n",
    "print(f\"Results stored in 'metrics_results' and 'performance_categories' variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e1acf3-3070-4c81-8e98-ac05e1cc9a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced visualization of reconstruction results for ABI satellite data\n",
    "def visualize_reconstruction(inputs, outputs, sample_idx=0, timestep=0, rgb_index=[1, 0, 2]):\n",
    "    \"\"\"Visualize original vs reconstructed for ABI satellite data\"\"\"\n",
    "    \n",
    "    original = inputs[sample_idx][timestep]  # (16, 512, 512) - 16 bands\n",
    "    reconstructed = outputs[sample_idx][timestep]  # (16, 512, 512) - 16 bands\n",
    "    \n",
    "    print(f\"Data shapes - Original: {original.shape}, Reconstructed: {reconstructed.shape}\")\n",
    "    print(f\"Data ranges - Original: [{original.min():.3f}, {original.max():.3f}], \"\n",
    "          f\"Reconstructed: [{reconstructed.min():.3f}, {reconstructed.max():.3f}]\")\n",
    "    \n",
    "    # Create RGB images using ABI bands\n",
    "    original_rgb = original[rgb_index].permute(1, 2, 0).numpy()  # (512, 512, 3)\n",
    "    reconstructed_rgb = reconstructed[rgb_index].permute(1, 2, 0).numpy()  # (512, 512, 3)\n",
    "    \n",
    "    # Better normalization for ABI data\n",
    "    def normalize_abi_rgb(rgb_data):\n",
    "        \"\"\"Normalize ABI RGB data for better visualization\"\"\"\n",
    "        normalized = np.zeros_like(rgb_data)\n",
    "        for i in range(3):\n",
    "            channel = rgb_data[:, :, i]\n",
    "            # Use percentile normalization to handle outliers\n",
    "            p2, p98 = np.percentile(channel, [2, 98])\n",
    "            normalized[:, :, i] = np.clip((channel - p2) / (p98 - p2), 0, 1)\n",
    "        return normalized\n",
    "    \n",
    "    original_display = normalize_abi_rgb(original_rgb)\n",
    "    reconstructed_display = normalize_abi_rgb(reconstructed_rgb)\n",
    "    \n",
    "    diff = np.abs(original_display - reconstructed_display)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # Top row: RGB composites\n",
    "    axes[0, 0].imshow(original_display)\n",
    "    axes[0, 0].set_title(f'Original RGB (Sample {sample_idx}, Time {timestep})\\nBands {rgb_index}')\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    axes[0, 1].imshow(reconstructed_display)\n",
    "    axes[0, 1].set_title(f'Reconstructed RGB\\nBands {rgb_index}')\n",
    "    axes[0, 1].axis('off')\n",
    "    \n",
    "    im_diff = axes[0, 2].imshow(diff, cmap='hot')\n",
    "    axes[0, 2].set_title(f'RGB Difference\\nMean: {np.mean(diff):.4f}')\n",
    "    axes[0, 2].axis('off')\n",
    "    plt.colorbar(im_diff, ax=axes[0, 2], fraction=0.046, pad=0.04)\n",
    "    \n",
    "   \n",
    "    for i, (band_idx, color_name) in enumerate(zip(rgb_index, ['Red', 'Green', 'Blue'])):\n",
    "        orig_band = original[band_idx].numpy()\n",
    "        recon_band = reconstructed[band_idx].numpy()\n",
    "        \n",
    "        orig_norm = (orig_band - np.percentile(orig_band, 2)) / (np.percentile(orig_band, 98) - np.percentile(orig_band, 2))\n",
    "        recon_norm = (recon_band - np.percentile(recon_band, 2)) / (np.percentile(recon_band, 98) - np.percentile(recon_band, 2))\n",
    "        \n",
    "        orig_norm = np.clip(orig_norm, 0, 1)\n",
    "        recon_norm = np.clip(recon_norm, 0, 1)\n",
    "        \n",
    "        band_diff = np.abs(orig_norm - recon_norm)\n",
    "        \n",
    "        # Create side-by-side comparison\n",
    "        combined = np.hstack([orig_norm, recon_norm, band_diff])\n",
    "        \n",
    "        im = axes[1, i].imshow(combined, cmap='viridis')\n",
    "        axes[1, i].set_title(f'Band {band_idx} ({color_name})\\nOrig | Recon | Diff')\n",
    "        axes[1, i].axis('off')\n",
    "        \n",
    "        # Add vertical lines to separate sections\n",
    "        h, w = orig_norm.shape\n",
    "        axes[1, i].axvline(x=w-0.5, color='white', linewidth=2)\n",
    "        axes[1, i].axvline(x=2*w-0.5, color='white', linewidth=2)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print quantitative metrics\n",
    "    mse = np.mean((original.numpy() - reconstructed.numpy()) ** 2)\n",
    "    rgb_mse = np.mean((original_display - reconstructed_display) ** 2)\n",
    "    \n",
    "    print(f\"\\nQuantitative Metrics:\")\n",
    "    print(f\"  Overall MSE (all bands): {mse:.6f}\")\n",
    "    print(f\"  RGB MSE: {rgb_mse:.6f}\")\n",
    "    print(f\"  RGB Mean Abs Diff: {np.mean(diff):.4f}\")\n",
    "    print(f\"  RGB Max Abs Diff: {np.max(diff):.4f}\")\n",
    "    print(f\"  Pixels with >10% difference: {np.sum(diff > 0.1) / diff.size * 100:.1f}%\")\n",
    "\n",
    "# Show reconstructions for first few samples\n",
    "if len(inputs) > 0:\n",
    "    print(\"ABI Satellite Data Reconstruction Results:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for i in range(min(3, len(inputs))):\n",
    "        print(f\"\\n--- Sample {i} ---\")\n",
    "        visualize_reconstruction(inputs, outputs, sample_idx=i, timestep=0, rgb_index=[1, 0, 2])\n",
    "        \n",
    "else:\n",
    "    print(\"No reconstruction data available for visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04670dc-477a-4148-8d57-924c3b9b56ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple stratification analysis\n",
    "def categorize_by_performance(losses):\n",
    "    \"\"\"Categorize samples by reconstruction difficulty\"\"\"\n",
    "    sorted_indices = np.argsort(losses)\n",
    "    n = len(losses)\n",
    "    \n",
    "    categories = {\n",
    "        'easy': sorted_indices[:n//3],\n",
    "        'medium': sorted_indices[n//3:2*n//3],\n",
    "        'hard': sorted_indices[2*n//3:]\n",
    "    }\n",
    "    \n",
    "    return categories\n",
    "\n",
    "def analyze_by_category(inputs, outputs, losses, categories):\n",
    "    \"\"\"Analyze performance by category\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for cat_name, indices in categories.items():\n",
    "        cat_mse = []\n",
    "        cat_psnr = []\n",
    "        \n",
    "        for idx in indices:\n",
    "            mse = calculate_mse(inputs[idx], outputs[idx])\n",
    "            psnr = calculate_psnr(mse)\n",
    "            cat_mse.append(mse)\n",
    "            cat_psnr.append(psnr)\n",
    "        \n",
    "        results[cat_name] = {\n",
    "            'count': len(indices),\n",
    "            'avg_mse': np.mean(cat_mse),\n",
    "            'std_mse': np.std(cat_mse),\n",
    "            'avg_psnr': np.mean(cat_psnr),\n",
    "            'std_psnr': np.std(cat_psnr),\n",
    "            'avg_loss': np.mean([losses[i] for i in indices])\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run analysis\n",
    "categories = categorize_by_performance(losses)\n",
    "results = analyze_by_category(inputs, outputs, losses, categories)\n",
    "\n",
    "# Print results\n",
    "print(\"Performance Analysis by Category:\")\n",
    "print(\"=\" * 50)\n",
    "for cat_name, metrics in results.items():\n",
    "    print(f\"{cat_name.upper()} Category:\")\n",
    "    print(f\"  Count: {metrics['count']}\")\n",
    "    print(f\"  Avg MSE: {metrics['avg_mse']:.4f} ± {metrics['std_mse']:.4f}\")\n",
    "    print(f\"  Avg PSNR: {metrics['avg_psnr']:.2f} ± {metrics['std_psnr']:.2f}\")\n",
    "    print(f\"  Avg Loss: {metrics['avg_loss']:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663f5ebf-b5b4-4046-92fd-df7ae108cd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Test if performance differences are significant\n",
    "def test_category_differences(inputs, outputs, categories):\n",
    "    \"\"\"Test if categories have significantly different performance\"\"\"\n",
    "    \n",
    "    # Get MSE for each category\n",
    "    easy_mse = [calculate_mse(inputs[i], outputs[i]) for i in categories['easy']]\n",
    "    medium_mse = [calculate_mse(inputs[i], outputs[i]) for i in categories['medium']]\n",
    "    hard_mse = [calculate_mse(inputs[i], outputs[i]) for i in categories['hard']]\n",
    "    \n",
    "    # ANOVA test\n",
    "    f_stat, p_value = stats.f_oneway(easy_mse, medium_mse, hard_mse)\n",
    "    \n",
    "    print(\"Statistical Testing Results:\")\n",
    "    print(f\"ANOVA F-statistic: {f_stat:.3f}\")\n",
    "    print(f\"ANOVA p-value: {p_value:.3f}\")\n",
    "    \n",
    "    if p_value < 0.05:\n",
    "        print(\"✅ Categories have significantly different performance\")\n",
    "    else:\n",
    "        print(\"❌ No significant difference between categories\")\n",
    "    \n",
    "    # Pairwise t-tests\n",
    "    pairs = [('easy', 'medium'), ('easy', 'hard'), ('medium', 'hard')]\n",
    "    mse_data = {'easy': easy_mse, 'medium': medium_mse, 'hard': hard_mse}\n",
    "    \n",
    "    print(\"\\nPairwise comparisons:\")\n",
    "    for cat1, cat2 in pairs:\n",
    "        t_stat, p_val = stats.ttest_ind(mse_data[cat1], mse_data[cat2])\n",
    "        significance = \"✅ Significant\" if p_val < 0.05 else \"❌ Not significant\"\n",
    "        print(f\"{cat1} vs {cat2}: t={t_stat:.3f}, p={p_val:.3f} - {significance}\")\n",
    "\n",
    "# Run statistical tests\n",
    "test_category_differences(inputs, outputs, categories)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:soehrle-satvis_kernel]",
   "language": "python",
   "name": "conda-env-soehrle-satvis_kernel-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
