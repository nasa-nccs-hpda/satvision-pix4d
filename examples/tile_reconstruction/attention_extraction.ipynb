{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec072dfa-bc65-477f-94c4-b7c93a51279c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPLETE ATTENTION EXTRACTION CODE\n",
    "\n",
    "import sys\n",
    "sys.path.append('/explore/nobackup/people/jacaraba/development/satvision-pix4d')\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import inspect\n",
    "from scipy import ndimage\n",
    "\n",
    "from satvision_pix4d.configs.config import _C, _update_config_from_file\n",
    "from satvision_pix4d.pipelines import PIPELINES\n",
    "from satvision_pix4d.datasets.abi_temporal_dataset import ABITemporalDataset\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d3e65e-0c6c-4ef3-8144-0f225404e9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the trained model and configuration\n",
    "print(\"=\" * 50)\n",
    "print(\"STEP 1: LOADING MODEL\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "model_filename = '/explore/nobackup/projects/pix4dcloud/jacaraba/model_development/satmae/satmae_satvision_pix4d_pretrain-dev/satmae_satvision_pix4d_pretrain-dev/best-epoch=196-val_loss=47.0728.ckpt/checkpoint/mp_rank_00_model_states.pt'\n",
    "config_filename = '/explore/nobackup/people/jacaraba/development/satvision-pix4d/tests/configs/test_satmae_dev.yaml'\n",
    "\n",
    "# Load the configuration file and update it with model path\n",
    "config = _C.clone()\n",
    "_update_config_from_file(config, config_filename)\n",
    "print(\"Loaded configuration file.\")\n",
    "\n",
    "config.defrost()\n",
    "config.MODEL.PRETRAINED = model_filename\n",
    "config.OUTPUT = '.'\n",
    "config.freeze()\n",
    "print(\"Updated configuration file.\")\n",
    "\n",
    "# Create the pipeline and load the model weights\n",
    "pipeline = PIPELINES[config.PIPELINE]\n",
    "ptlPipeline = pipeline(config)\n",
    "\n",
    "checkpoint_data = torch.load(config.MODEL.PRETRAINED, map_location='cpu', weights_only=False)\n",
    "model = ptlPipeline.load_checkpoint(config.MODEL.PRETRAINED, config)\n",
    "model.cpu()\n",
    "model.eval()\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "print(f\"Model type: {type(model)}\")\n",
    "\n",
    "# Check what parameters the model expects\n",
    "sig = inspect.signature(model.forward)\n",
    "print(f\"Model forward parameters: {list(sig.parameters.keys())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d801cef2-e4f6-4923-8d4e-47b9c2104d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load satellite data for analysis\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"STEP 2: LOADING DATA\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "data_dir = '/explore/nobackup/people/jacaraba/projects/SatVision-Pix4d/tiles_pix4d/'\n",
    "\n",
    "# Create dataset with 16-channel satellite imagery\n",
    "dataset = ABITemporalDataset(\n",
    "    data_paths=[data_dir], \n",
    "    img_size=512,\n",
    "    in_chans=16,\n",
    "    data_var='Rad'\n",
    ")\n",
    "\n",
    "print(f\"Dataset created with {len(dataset)} samples\")\n",
    "\n",
    "# Load one batch of data\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "for batch in dataloader:\n",
    "    imgs = batch\n",
    "    break\n",
    "\n",
    "print(f\"Data loaded successfully!\")\n",
    "print(f\"imgs type: {type(imgs)}\")\n",
    "\n",
    "# Handle different data formats that might come from the dataset\n",
    "if isinstance(imgs, list):\n",
    "    print(f\"imgs is a list with {len(imgs)} elements\")\n",
    "    if len(imgs) > 0:\n",
    "        if isinstance(imgs[0], torch.Tensor):\n",
    "            try:\n",
    "                imgs = torch.stack(imgs)\n",
    "                print(f\"Converted list to tensor: {imgs.shape}\")\n",
    "            except:\n",
    "                imgs = imgs[0]\n",
    "                print(f\"Using first tensor from list: {imgs.shape}\")\n",
    "        else:\n",
    "            print(f\"First element type: {type(imgs[0])}\")\n",
    "            try:\n",
    "                imgs = torch.tensor(imgs)\n",
    "                print(f\"Converted to tensor: {imgs.shape}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Could not convert to tensor: {e}\")\n",
    "                imgs = None\n",
    "elif isinstance(imgs, torch.Tensor):\n",
    "    print(f\"imgs is already a tensor: {imgs.shape}\")\n",
    "else:\n",
    "    print(f\"Unexpected imgs type: {type(imgs)}\")\n",
    "    try:\n",
    "        imgs = torch.tensor(imgs)\n",
    "        print(f\"Forced conversion to tensor: {imgs.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not convert to tensor: {e}\")\n",
    "        imgs = None\n",
    "\n",
    "if imgs is not None:\n",
    "    print(f\"Final imgs shape: {imgs.shape}\")\n",
    "    print(f\"Final imgs type: {type(imgs)}\")\n",
    "else:\n",
    "    print(\"Failed to load imgs as tensor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fea496d-fa32-441e-b424-58d1f533c191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create timestamps and test model inference\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"STEP 3: CREATING TIMESTAMPS AND RUNNING INFERENCE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if imgs is None:\n",
    "    print(\"Cannot create timestamps - imgs is None\")\n",
    "    timestamps = None\n",
    "    pred_imgs = None\n",
    "else:\n",
    "    batch_size, time_steps = imgs.shape[0], imgs.shape[1]\n",
    "    print(f\"Batch size: {batch_size}, Time steps: {time_steps}\")\n",
    "\n",
    "    # Create simple sequential timestamps for the time series\n",
    "    timestamps = torch.arange(time_steps).unsqueeze(0).expand(batch_size, -1)\n",
    "    print(f\"timestamps shape: {timestamps.shape}\")\n",
    "    print(f\"timestamps: {timestamps}\")\n",
    "\n",
    "    # Try running the model to make sure everything works before attention extraction\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            print(\"Attempting model inference...\")\n",
    "            model_output = model(samples=imgs, timestamps=timestamps)\n",
    "            \n",
    "            # Handle different types of model outputs\n",
    "            if isinstance(model_output, tuple):\n",
    "                print(f\"Model returned tuple with {len(model_output)} elements\")\n",
    "                for i, output in enumerate(model_output):\n",
    "                    if hasattr(output, 'shape'):\n",
    "                        print(f\"  Output {i}: {output.shape}\")\n",
    "                    else:\n",
    "                        print(f\"  Output {i}: {type(output)}\")\n",
    "                \n",
    "                pred_imgs = model_output[0]\n",
    "                print(f\"Using first element as pred_imgs: {pred_imgs.shape}\")\n",
    "            else:\n",
    "                pred_imgs = model_output\n",
    "                print(f\"Model inference successful!\")\n",
    "                print(f\"pred_imgs shape: {pred_imgs.shape}\")\n",
    "                \n",
    "            print(f\"pred_imgs type: {type(pred_imgs)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Model inference failed: {e}\")\n",
    "            print(f\"Error type: {type(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            pred_imgs = None\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96728c4-62b9-4177-8b4a-e2588e5c1992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Define functions for extracting and visualizing attention\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"STEP 4: DEFINING ATTENTION EXTRACTION FUNCTIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def get_last_self_attention(model, samples, timestamps, return_attn=True):\n",
    "    \"\"\"\n",
    "    Extract attention weights from the model by trying different approaches\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Try approach 1: Check if model has a forward_encoder method with return_attn\n",
    "        if hasattr(model, 'model') and hasattr(model.model, 'forward_encoder'):\n",
    "            try:\n",
    "                sig = inspect.signature(model.model.forward_encoder)\n",
    "                if 'return_attn' in sig.parameters:\n",
    "                    encoded, attn_weights = model.model.forward_encoder(samples, timestamps, return_attn=True)\n",
    "                    print(f\"Got attention from forward_encoder: {attn_weights.shape}\")\n",
    "                    return attn_weights\n",
    "            except Exception as e:\n",
    "                print(f\"forward_encoder approach failed: {e}\")\n",
    "        \n",
    "        # Try approach 2: Hook into attention modules during forward pass\n",
    "        print(\"Searching for attention modules...\")\n",
    "        attention_modules = []\n",
    "        for name, module in model.named_modules():\n",
    "            module_name_lower = name.lower()\n",
    "            module_type = str(type(module)).lower()\n",
    "            \n",
    "            # Look for modules with attention-related names\n",
    "            if any(keyword in module_name_lower for keyword in ['attn', 'attention', 'self_attn']):\n",
    "                attention_modules.append((name, module))\n",
    "                print(f\"Found attention module: {name} ({type(module)})\")\n",
    "            elif any(keyword in module_type for keyword in ['attention', 'multihead']):\n",
    "                attention_modules.append((name, module))\n",
    "                print(f\"Found attention module by type: {name} ({type(module)})\")\n",
    "        \n",
    "        if not attention_modules:\n",
    "            print(\"No attention modules found\")\n",
    "            return None\n",
    "        \n",
    "        # Set up hooks to capture attention weights\n",
    "        attn_weights = []\n",
    "        \n",
    "        def attention_hook(module, input, output):\n",
    "            # Check if output contains attention weights (usually 4D tensor with square last two dims)\n",
    "            if isinstance(output, tuple):\n",
    "                for i, out in enumerate(output):\n",
    "                    if hasattr(out, 'shape') and len(out.shape) == 4:\n",
    "                        if out.shape[-1] == out.shape[-2] and out.shape[-1] > 1:\n",
    "                            print(f\"Captured attention from tuple element {i}: {out.shape}\")\n",
    "                            attn_weights.append(out)\n",
    "            elif hasattr(output, 'shape') and len(output.shape) == 4:\n",
    "                if output.shape[-1] == output.shape[-2] and output.shape[-1] > 1:\n",
    "                    print(f\"Captured attention from direct output: {output.shape}\")\n",
    "                    attn_weights.append(output)\n",
    "            \n",
    "            # Check if module stores attention weights as an attribute\n",
    "            if hasattr(module, 'attn_weights'):\n",
    "                print(f\"Found stored attention weights: {module.attn_weights.shape}\")\n",
    "                attn_weights.append(module.attn_weights)\n",
    "        \n",
    "        # Register hooks on all attention modules\n",
    "        hooks = []\n",
    "        for name, module in attention_modules:\n",
    "            hook = module.register_forward_hook(attention_hook)\n",
    "            hooks.append(hook)\n",
    "            print(f\"Registered hook on: {name}\")\n",
    "        \n",
    "        print(f\"Registered {len(hooks)} hooks\")\n",
    "        \n",
    "        # Run forward pass to trigger hooks\n",
    "        try:\n",
    "            output = model(samples=samples, timestamps=timestamps)\n",
    "            print(\"Forward pass completed for attention extraction\")\n",
    "        except Exception as e:\n",
    "            print(f\"Forward pass failed during attention extraction: {e}\")\n",
    "        \n",
    "        # Clean up hooks\n",
    "        for hook in hooks:\n",
    "            hook.remove()\n",
    "        \n",
    "        # Return the last captured attention weights (usually from the final layer)\n",
    "        if attn_weights:\n",
    "            print(f\"Found {len(attn_weights)} attention tensors\")\n",
    "            for i, attn in enumerate(attn_weights):\n",
    "                print(f\"  Attention {i}: {attn.shape}\")\n",
    "            return attn_weights[-1]\n",
    "        else:\n",
    "            print(\"No attention weights captured\")\n",
    "            \n",
    "            # Try approach 3: Direct access to transformer blocks\n",
    "            print(\"Trying direct access to transformer blocks...\")\n",
    "            for name, module in model.named_modules():\n",
    "                if hasattr(module, 'blocks'):\n",
    "                    print(f\"Found transformer blocks in: {name}\")\n",
    "                    try:\n",
    "                        if hasattr(module, 'forward_encoder'):\n",
    "                            result = module.forward_encoder(samples, return_attn=True)\n",
    "                            if isinstance(result, tuple) and len(result) > 1:\n",
    "                                return result[1]\n",
    "                    except:\n",
    "                        pass\n",
    "            \n",
    "            return None\n",
    "\n",
    "def visualize_attention_on_image(image, attn, patch_size=16, head=0, cls_token=True, channel=0):\n",
    "    \"\"\"\n",
    "    Overlay attention map on satellite image for visualization\n",
    "    \"\"\"\n",
    "    # Convert image to numpy if needed\n",
    "    if isinstance(image, torch.Tensor):\n",
    "        image = image.detach().cpu().numpy()\n",
    "    \n",
    "    if len(image.shape) == 4:\n",
    "        image = image[0]\n",
    "    \n",
    "    # Create RGB visualization from multi-channel satellite data\n",
    "    if len(image.shape) == 3:\n",
    "        if image.shape[0] >= 3:\n",
    "            # Use channels 0, 5, 10 as RGB approximation\n",
    "            rgb_channels = [0, 5, min(10, image.shape[0]-1)]\n",
    "            display_image = np.zeros((image.shape[1], image.shape[2], 3))\n",
    "            for i, ch in enumerate(rgb_channels):\n",
    "                channel_data = image[ch]\n",
    "                # Normalize using percentiles for better visualization\n",
    "                p2, p98 = np.percentile(channel_data, [2, 98])\n",
    "                if p98 > p2:\n",
    "                    display_image[:, :, i] = np.clip((channel_data - p2) / (p98 - p2), 0, 1)\n",
    "        else:\n",
    "            # Single channel - convert to grayscale RGB\n",
    "            display_image = image[channel]\n",
    "            p2, p98 = np.percentile(display_image, [2, 98])\n",
    "            if p98 > p2:\n",
    "                display_image = np.clip((display_image - p2) / (p98 - p2), 0, 1)\n",
    "            display_image = np.stack([display_image] * 3, axis=-1)\n",
    "    else:\n",
    "        display_image = image\n",
    "    \n",
    "    # Process attention weights\n",
    "    if isinstance(attn, torch.Tensor):\n",
    "        attn = attn.detach().cpu().numpy()\n",
    "    \n",
    "    attn_head = attn[head]\n",
    "    \n",
    "    # Use CLS token attention or average attention\n",
    "    if cls_token:\n",
    "        attn_map = attn_head[0, 1:]  # Skip CLS token in target\n",
    "    else:\n",
    "        attn_map = attn_head.mean(axis=0)[1:]\n",
    "    \n",
    "    # Reshape attention to spatial grid\n",
    "    num_patches = attn_map.shape[0]\n",
    "    grid_size = int(np.sqrt(num_patches))\n",
    "    \n",
    "    if grid_size * grid_size != num_patches:\n",
    "        print(f\"Warning: Cannot reshape {num_patches} patches into square grid\")\n",
    "        grid_size = int(np.sqrt(num_patches))\n",
    "        attn_map = attn_map[:grid_size*grid_size]\n",
    "    \n",
    "    attn_map = attn_map.reshape(grid_size, grid_size)\n",
    "    \n",
    "    # Normalize attention map\n",
    "    if attn_map.max() > attn_map.min():\n",
    "        attn_map = (attn_map - attn_map.min()) / (attn_map.max() - attn_map.min())\n",
    "    \n",
    "    # Upsample attention map to match image size\n",
    "    attn_map = np.kron(attn_map, np.ones((patch_size, patch_size)))\n",
    "    \n",
    "    # Resize to exactly match image dimensions\n",
    "    img_h, img_w = display_image.shape[:2]\n",
    "    if attn_map.shape[0] != img_h or attn_map.shape[1] != img_w:\n",
    "        attn_map = ndimage.zoom(attn_map, (img_h/attn_map.shape[0], img_w/attn_map.shape[1]))\n",
    "    \n",
    "    # Create visualization with three panels\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    axes[0].imshow(display_image)\n",
    "    axes[0].set_title('Original Satellite Image')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    im1 = axes[1].imshow(attn_map, cmap='jet')\n",
    "    axes[1].set_title(f'Attention Map (Head {head})')\n",
    "    axes[1].axis('off')\n",
    "    plt.colorbar(im1, ax=axes[1])\n",
    "    \n",
    "    axes[2].imshow(display_image)\n",
    "    axes[2].imshow(attn_map, cmap='jet', alpha=0.4)\n",
    "    axes[2].set_title('Attention Overlay')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return attn_map\n",
    "\n",
    "def visualize_multiple_heads(image, attn, patch_size=16, num_heads=8):\n",
    "    \"\"\"\n",
    "    Show attention patterns from multiple attention heads\n",
    "    \"\"\"\n",
    "    if isinstance(attn, torch.Tensor):\n",
    "        attn = attn.detach().cpu().numpy()\n",
    "    \n",
    "    num_heads = min(num_heads, attn.shape[0])\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for head in range(num_heads):\n",
    "        attn_head = attn[head]\n",
    "        attn_map = attn_head[0, 1:]  # CLS token attention\n",
    "        \n",
    "        # Reshape to spatial grid\n",
    "        num_patches = attn_map.shape[0]\n",
    "        grid_size = int(np.sqrt(num_patches))\n",
    "        if grid_size * grid_size != num_patches:\n",
    "            grid_size = int(np.sqrt(num_patches))\n",
    "            attn_map = attn_map[:grid_size*grid_size]\n",
    "        \n",
    "        attn_map = attn_map.reshape(grid_size, grid_size)\n",
    "        \n",
    "        # Normalize\n",
    "        if attn_map.max() > attn_map.min():\n",
    "            attn_map = (attn_map - attn_map.min()) / (attn_map.max() - attn_map.min())\n",
    "        \n",
    "        if head < len(axes):\n",
    "            im = axes[head].imshow(attn_map, cmap='jet')\n",
    "            axes[head].set_title(f'Head {head}')\n",
    "            axes[head].axis('off')\n",
    "            plt.colorbar(im, ax=axes[head])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def extract_and_visualize_attention(model, imgs, timestamps, sample_idx=0, timestep=0):\n",
    "    \"\"\"\n",
    "    Main function to extract attention and create visualizations\n",
    "    \"\"\"\n",
    "    print(\"Extracting attention weights...\")\n",
    "    print(f\"Input shape: {imgs.shape}\")\n",
    "    print(f\"Timestamps shape: {timestamps.shape}\")\n",
    "    \n",
    "    attn_weights = get_last_self_attention(model, imgs, timestamps)\n",
    "    \n",
    "    if attn_weights is not None:\n",
    "        print(f\"Attention weights extracted: {attn_weights.shape}\")\n",
    "        \n",
    "        # Get the image for visualization\n",
    "        viz_image = imgs[sample_idx, timestep]\n",
    "        print(f\"Visualization image shape: {viz_image.shape}\")\n",
    "        \n",
    "        # Create single head visualization\n",
    "        print(\"Creating attention visualization...\")\n",
    "        attn_map = visualize_attention_on_image(\n",
    "            viz_image, \n",
    "            attn_weights[sample_idx], \n",
    "            patch_size=16, \n",
    "            head=0\n",
    "        )\n",
    "        \n",
    "        # Create multi-head visualization if multiple heads exist\n",
    "        if attn_weights.shape[1] > 1:\n",
    "            print(\"Visualizing multiple attention heads...\")\n",
    "            visualize_multiple_heads(\n",
    "                viz_image, \n",
    "                attn_weights[sample_idx], \n",
    "                patch_size=16, \n",
    "                num_heads=min(8, attn_weights.shape[1])\n",
    "            )\n",
    "        \n",
    "        return attn_weights\n",
    "    else:\n",
    "        print(\"Could not extract attention weights from model\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea072677-9e88-49af-8fdf-18cbffe6dc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Run the complete attention extraction and visualization pipeline\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"STEP 5: RUNNING ATTENTION EXTRACTION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check that all required variables are available\n",
    "print(\"Checking variables...\")\n",
    "variables_ok = True\n",
    "\n",
    "if 'model' not in locals():\n",
    "    print(\"model not found\")\n",
    "    variables_ok = False\n",
    "else:\n",
    "    print(f\"model: {type(model)}\")\n",
    "\n",
    "if 'imgs' not in locals():\n",
    "    print(\"imgs not found\") \n",
    "    variables_ok = False\n",
    "elif imgs is None:\n",
    "    print(\"imgs is None\")\n",
    "    variables_ok = False\n",
    "else:\n",
    "    print(f\"imgs: {imgs.shape}\")\n",
    "\n",
    "if 'timestamps' not in locals():\n",
    "    print(\"timestamps not found\")\n",
    "    variables_ok = False\n",
    "elif timestamps is None:\n",
    "    print(\"timestamps is None\")\n",
    "    variables_ok = False\n",
    "else:\n",
    "    print(f\"timestamps: {timestamps.shape}\")\n",
    "\n",
    "if 'pred_imgs' not in locals():\n",
    "    print(\"pred_imgs not found (but we can work without it)\")\n",
    "elif pred_imgs is None:\n",
    "    print(\"pred_imgs is None (model inference failed, but we can still extract attention)\")\n",
    "else:\n",
    "    print(f\"pred_imgs: {pred_imgs.shape}\")\n",
    "\n",
    "# Run attention extraction if everything looks good\n",
    "if variables_ok:\n",
    "    print(\"\\nRUNNING ATTENTION EXTRACTION...\")\n",
    "    attention_weights = extract_and_visualize_attention(\n",
    "        model, \n",
    "        imgs, \n",
    "        timestamps, \n",
    "        sample_idx=0, \n",
    "        timestep=0\n",
    "    )\n",
    "    \n",
    "    if attention_weights is not None:\n",
    "        print(\"\\nSUCCESS! Attention extraction completed.\")\n",
    "        print(f\"Final attention shape: {attention_weights.shape}\")\n",
    "    else:\n",
    "        print(\"\\nAttention extraction failed.\")\n",
    "else:\n",
    "    print(\"Missing required variables - check the steps above for errors\")\n",
    "    print(\"\\nDebugging info:\")\n",
    "    print(f\"  model exists: {'model' in locals()}\")\n",
    "    print(f\"  imgs exists: {'imgs' in locals()}\")\n",
    "    print(f\"  imgs is not None: {imgs is not None if 'imgs' in locals() else 'N/A'}\")\n",
    "    print(f\"  timestamps exists: {'timestamps' in locals()}\")\n",
    "    print(f\"  timestamps is not None: {timestamps is not None if 'timestamps' in locals() else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc84091-784f-4ffc-9878-8c8402e38cdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:soehrle-satvis_kernel]",
   "language": "python",
   "name": "conda-env-soehrle-satvis_kernel-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
