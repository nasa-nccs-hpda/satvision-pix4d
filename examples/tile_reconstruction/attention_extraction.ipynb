{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf18505c-990b-41df-a003-54055aa90b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "STEP 1: LOADING MODEL\n",
      "==================================================\n",
      "Loaded configuration file.\n",
      "Updated configuration file.\n",
      "Model loaded successfully!\n",
      "Model type: <class 'satvision_pix4d.pipelines.satvision_pix4d_pretrain.SatVisionPix4DSatMAEPretrain'>\n",
      "Model forward parameters: ['samples', 'timestamps']\n",
      "\n",
      "==================================================\n",
      "STEP 2: LOADING DATA\n",
      "==================================================\n",
      "Dataset created with 889 samples\n",
      "Data loaded successfully!\n",
      "imgs type: <class 'list'>\n",
      "imgs is a list with 2 elements\n",
      "Using first tensor from list: torch.Size([1, 6, 16, 512, 512])\n",
      "Final imgs shape: torch.Size([1, 6, 16, 512, 512])\n",
      "Final imgs type: <class 'torch.Tensor'>\n",
      "\n",
      "==================================================\n",
      "STEP 3: CREATING TIMESTAMPS AND RUNNING INFERENCE\n",
      "==================================================\n",
      "Batch size: 1, Time steps: 6\n",
      "timestamps shape: torch.Size([1, 6])\n",
      "timestamps: tensor([[0, 1, 2, 3, 4, 5]])\n",
      "Attempting model inference...\n",
      "Model returned tuple with 3 elements\n",
      "  Output 0: torch.Size([])\n",
      "  Output 1: torch.Size([1, 6144, 4096])\n",
      "  Output 2: torch.Size([1, 6144])\n",
      "Using first element as pred_imgs: torch.Size([])\n",
      "pred_imgs type: <class 'torch.Tensor'>\n",
      "\n",
      "==================================================\n",
      "STEP 4: DEFINING ATTENTION EXTRACTION FUNCTIONS\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "STEP 5: RUNNING ATTENTION EXTRACTION\n",
      "==================================================\n",
      "Checking variables...\n",
      "model: <class 'satvision_pix4d.pipelines.satvision_pix4d_pretrain.SatVisionPix4DSatMAEPretrain'>\n",
      "imgs: torch.Size([1, 6, 16, 512, 512])\n",
      "timestamps: torch.Size([1, 6])\n",
      "pred_imgs: torch.Size([])\n",
      "\n",
      "RUNNING ATTENTION EXTRACTION...\n",
      "Extracting attention weights...\n",
      "Input shape: torch.Size([1, 6, 16, 512, 512])\n",
      "Timestamps shape: torch.Size([1, 6])\n",
      "Searching for attention modules...\n",
      "Found attention module: model.blocks.0.attn (<class 'satvision_pix4d.models.encoders.models_mae_temporal.FlashMHAWrapper'>)\n",
      "Found attention module: model.blocks.0.attn.attn (<class 'flash_attn.modules.mha.MHA'>)\n",
      "Found attention module: model.blocks.0.attn.attn.Wqkv (<class 'torch.nn.modules.linear.Linear'>)\n",
      "Found attention module: model.blocks.0.attn.attn.inner_attn (<class 'flash_attn.modules.mha.SelfAttention'>)\n",
      "Found attention module: model.blocks.0.attn.attn.inner_attn.drop (<class 'torch.nn.modules.dropout.Dropout'>)\n",
      "Found attention module: model.blocks.0.attn.attn.inner_cross_attn (<class 'flash_attn.modules.mha.CrossAttention'>)\n",
      "Found attention module: model.blocks.0.attn.attn.inner_cross_attn.drop (<class 'torch.nn.modules.dropout.Dropout'>)\n",
      "Found attention module: model.blocks.0.attn.attn.out_proj (<class 'torch.nn.modules.linear.Linear'>)\n",
      "Found attention module: model.blocks.1.attn (<class 'satvision_pix4d.models.encoders.models_mae_temporal.FlashMHAWrapper'>)\n",
      "Found attention module: model.blocks.1.attn.attn (<class 'flash_attn.modules.mha.MHA'>)\n",
      "Found attention module: model.blocks.1.attn.attn.Wqkv (<class 'torch.nn.modules.linear.Linear'>)\n",
      "Found attention module: model.blocks.1.attn.attn.inner_attn (<class 'flash_attn.modules.mha.SelfAttention'>)\n",
      "Found attention module: model.blocks.1.attn.attn.inner_attn.drop (<class 'torch.nn.modules.dropout.Dropout'>)\n",
      "Found attention module: model.blocks.1.attn.attn.inner_cross_attn (<class 'flash_attn.modules.mha.CrossAttention'>)\n",
      "Found attention module: model.blocks.1.attn.attn.inner_cross_attn.drop (<class 'torch.nn.modules.dropout.Dropout'>)\n",
      "Found attention module: model.blocks.1.attn.attn.out_proj (<class 'torch.nn.modules.linear.Linear'>)\n",
      "Found attention module: model.blocks.2.attn (<class 'satvision_pix4d.models.encoders.models_mae_temporal.FlashMHAWrapper'>)\n",
      "Found attention module: model.blocks.2.attn.attn (<class 'flash_attn.modules.mha.MHA'>)\n",
      "Found attention module: model.blocks.2.attn.attn.Wqkv (<class 'torch.nn.modules.linear.Linear'>)\n",
      "Found attention module: model.blocks.2.attn.attn.inner_attn (<class 'flash_attn.modules.mha.SelfAttention'>)\n",
      "Found attention module: model.blocks.2.attn.attn.inner_attn.drop (<class 'torch.nn.modules.dropout.Dropout'>)\n",
      "Found attention module: model.blocks.2.attn.attn.inner_cross_attn (<class 'flash_attn.modules.mha.CrossAttention'>)\n",
      "Found attention module: model.blocks.2.attn.attn.inner_cross_attn.drop (<class 'torch.nn.modules.dropout.Dropout'>)\n",
      "Found attention module: model.blocks.2.attn.attn.out_proj (<class 'torch.nn.modules.linear.Linear'>)\n",
      "Found attention module: model.blocks.3.attn (<class 'satvision_pix4d.models.encoders.models_mae_temporal.FlashMHAWrapper'>)\n",
      "Found attention module: model.blocks.3.attn.attn (<class 'flash_attn.modules.mha.MHA'>)\n",
      "Found attention module: model.blocks.3.attn.attn.Wqkv (<class 'torch.nn.modules.linear.Linear'>)\n",
      "Found attention module: model.blocks.3.attn.attn.inner_attn (<class 'flash_attn.modules.mha.SelfAttention'>)\n",
      "Found attention module: model.blocks.3.attn.attn.inner_attn.drop (<class 'torch.nn.modules.dropout.Dropout'>)\n",
      "Found attention module: model.blocks.3.attn.attn.inner_cross_attn (<class 'flash_attn.modules.mha.CrossAttention'>)\n",
      "Found attention module: model.blocks.3.attn.attn.inner_cross_attn.drop (<class 'torch.nn.modules.dropout.Dropout'>)\n",
      "Found attention module: model.blocks.3.attn.attn.out_proj (<class 'torch.nn.modules.linear.Linear'>)\n",
      "Found attention module: model.blocks.4.attn (<class 'satvision_pix4d.models.encoders.models_mae_temporal.FlashMHAWrapper'>)\n",
      "Found attention module: model.blocks.4.attn.attn (<class 'flash_attn.modules.mha.MHA'>)\n",
      "Found attention module: model.blocks.4.attn.attn.Wqkv (<class 'torch.nn.modules.linear.Linear'>)\n",
      "Found attention module: model.blocks.4.attn.attn.inner_attn (<class 'flash_attn.modules.mha.SelfAttention'>)\n",
      "Found attention module: model.blocks.4.attn.attn.inner_attn.drop (<class 'torch.nn.modules.dropout.Dropout'>)\n",
      "Found attention module: model.blocks.4.attn.attn.inner_cross_attn (<class 'flash_attn.modules.mha.CrossAttention'>)\n",
      "Found attention module: model.blocks.4.attn.attn.inner_cross_attn.drop (<class 'torch.nn.modules.dropout.Dropout'>)\n",
      "Found attention module: model.blocks.4.attn.attn.out_proj (<class 'torch.nn.modules.linear.Linear'>)\n",
      "Found attention module: model.blocks.5.attn (<class 'satvision_pix4d.models.encoders.models_mae_temporal.FlashMHAWrapper'>)\n",
      "Found attention module: model.blocks.5.attn.attn (<class 'flash_attn.modules.mha.MHA'>)\n",
      "Found attention module: model.blocks.5.attn.attn.Wqkv (<class 'torch.nn.modules.linear.Linear'>)\n",
      "Found attention module: model.blocks.5.attn.attn.inner_attn (<class 'flash_attn.modules.mha.SelfAttention'>)\n",
      "Found attention module: model.blocks.5.attn.attn.inner_attn.drop (<class 'torch.nn.modules.dropout.Dropout'>)\n",
      "Found attention module: model.blocks.5.attn.attn.inner_cross_attn (<class 'flash_attn.modules.mha.CrossAttention'>)\n",
      "Found attention module: model.blocks.5.attn.attn.inner_cross_attn.drop (<class 'torch.nn.modules.dropout.Dropout'>)\n",
      "Found attention module: model.blocks.5.attn.attn.out_proj (<class 'torch.nn.modules.linear.Linear'>)\n",
      "Found attention module: model.blocks.6.attn (<class 'satvision_pix4d.models.encoders.models_mae_temporal.FlashMHAWrapper'>)\n",
      "Found attention module: model.blocks.6.attn.attn (<class 'flash_attn.modules.mha.MHA'>)\n",
      "Found attention module: model.blocks.6.attn.attn.Wqkv (<class 'torch.nn.modules.linear.Linear'>)\n",
      "Found attention module: model.blocks.6.attn.attn.inner_attn (<class 'flash_attn.modules.mha.SelfAttention'>)\n",
      "Found attention module: model.blocks.6.attn.attn.inner_attn.drop (<class 'torch.nn.modules.dropout.Dropout'>)\n",
      "Found attention module: model.blocks.6.attn.attn.inner_cross_attn (<class 'flash_attn.modules.mha.CrossAttention'>)\n",
      "Found attention module: model.blocks.6.attn.attn.inner_cross_attn.drop (<class 'torch.nn.modules.dropout.Dropout'>)\n",
      "Found attention module: model.blocks.6.attn.attn.out_proj (<class 'torch.nn.modules.linear.Linear'>)\n",
      "Found attention module: model.blocks.7.attn (<class 'satvision_pix4d.models.encoders.models_mae_temporal.FlashMHAWrapper'>)\n",
      "Found attention module: model.blocks.7.attn.attn (<class 'flash_attn.modules.mha.MHA'>)\n",
      "Found attention module: model.blocks.7.attn.attn.Wqkv (<class 'torch.nn.modules.linear.Linear'>)\n",
      "Found attention module: model.blocks.7.attn.attn.inner_attn (<class 'flash_attn.modules.mha.SelfAttention'>)\n",
      "Found attention module: model.blocks.7.attn.attn.inner_attn.drop (<class 'torch.nn.modules.dropout.Dropout'>)\n",
      "Found attention module: model.blocks.7.attn.attn.inner_cross_attn (<class 'flash_attn.modules.mha.CrossAttention'>)\n",
      "Found attention module: model.blocks.7.attn.attn.inner_cross_attn.drop (<class 'torch.nn.modules.dropout.Dropout'>)\n",
      "Found attention module: model.blocks.7.attn.attn.out_proj (<class 'torch.nn.modules.linear.Linear'>)\n",
      "Found attention module: model.blocks.8.attn (<class 'satvision_pix4d.models.encoders.models_mae_temporal.FlashMHAWrapper'>)\n",
      "Found attention module: model.blocks.8.attn.attn (<class 'flash_attn.modules.mha.MHA'>)\n",
      "Found attention module: model.blocks.8.attn.attn.Wqkv (<class 'torch.nn.modules.linear.Linear'>)\n",
      "Found attention module: model.blocks.8.attn.attn.inner_attn (<class 'flash_attn.modules.mha.SelfAttention'>)\n",
      "Found attention module: model.blocks.8.attn.attn.inner_attn.drop (<class 'torch.nn.modules.dropout.Dropout'>)\n",
      "Found attention module: model.blocks.8.attn.attn.inner_cross_attn (<class 'flash_attn.modules.mha.CrossAttention'>)\n",
      "Found attention module: model.blocks.8.attn.attn.inner_cross_attn.drop (<class 'torch.nn.modules.dropout.Dropout'>)\n",
      "Found attention module: model.blocks.8.attn.attn.out_proj (<class 'torch.nn.modules.linear.Linear'>)\n",
      "Found attention module: model.blocks.9.attn (<class 'satvision_pix4d.models.encoders.models_mae_temporal.FlashMHAWrapper'>)\n",
      "Found attention module: model.blocks.9.attn.attn (<class 'flash_attn.modules.mha.MHA'>)\n",
      "Found attention module: model.blocks.9.attn.attn.Wqkv (<class 'torch.nn.modules.linear.Linear'>)\n",
      "Found attention module: model.blocks.9.attn.attn.inner_attn (<class 'flash_attn.modules.mha.SelfAttention'>)\n",
      "Found attention module: model.blocks.9.attn.attn.inner_attn.drop (<class 'torch.nn.modules.dropout.Dropout'>)\n",
      "Found attention module: model.blocks.9.attn.attn.inner_cross_attn (<class 'flash_attn.modules.mha.CrossAttention'>)\n",
      "Found attention module: model.blocks.9.attn.attn.inner_cross_attn.drop (<class 'torch.nn.modules.dropout.Dropout'>)\n",
      "Found attention module: model.blocks.9.attn.attn.out_proj (<class 'torch.nn.modules.linear.Linear'>)\n",
      "Found attention module: model.blocks.10.attn (<class 'satvision_pix4d.models.encoders.models_mae_temporal.FlashMHAWrapper'>)\n",
      "Found attention module: model.blocks.10.attn.attn (<class 'flash_attn.modules.mha.MHA'>)\n",
      "Found attention module: model.blocks.10.attn.attn.Wqkv (<class 'torch.nn.modules.linear.Linear'>)\n",
      "Found attention module: model.blocks.10.attn.attn.inner_attn (<class 'flash_attn.modules.mha.SelfAttention'>)\n",
      "Found attention module: model.blocks.10.attn.attn.inner_attn.drop (<class 'torch.nn.modules.dropout.Dropout'>)\n",
      "Found attention module: model.blocks.10.attn.attn.inner_cross_attn (<class 'flash_attn.modules.mha.CrossAttention'>)\n",
      "Found attention module: model.blocks.10.attn.attn.inner_cross_attn.drop (<class 'torch.nn.modules.dropout.Dropout'>)\n",
      "Found attention module: model.blocks.10.attn.attn.out_proj (<class 'torch.nn.modules.linear.Linear'>)\n",
      "Found attention module: model.blocks.11.attn (<class 'satvision_pix4d.models.encoders.models_mae_temporal.FlashMHAWrapper'>)\n",
      "Found attention module: model.blocks.11.attn.attn (<class 'flash_attn.modules.mha.MHA'>)\n",
      "Found attention module: model.blocks.11.attn.attn.Wqkv (<class 'torch.nn.modules.linear.Linear'>)\n",
      "Found attention module: model.blocks.11.attn.attn.inner_attn (<class 'flash_attn.modules.mha.SelfAttention'>)\n",
      "Found attention module: model.blocks.11.attn.attn.inner_attn.drop (<class 'torch.nn.modules.dropout.Dropout'>)\n",
      "Found attention module: model.blocks.11.attn.attn.inner_cross_attn (<class 'flash_attn.modules.mha.CrossAttention'>)\n",
      "Found attention module: model.blocks.11.attn.attn.inner_cross_attn.drop (<class 'torch.nn.modules.dropout.Dropout'>)\n",
      "Found attention module: model.blocks.11.attn.attn.out_proj (<class 'torch.nn.modules.linear.Linear'>)\n",
      "Found attention module: model.blocks.12.attn (<class 'satvision_pix4d.models.encoders.models_mae_temporal.FlashMHAWrapper'>)\n",
      "Found attention module: model.blocks.12.attn.attn (<class 'flash_attn.modules.mha.MHA'>)\n",
      "Found attention module: model.blocks.12.attn.attn.Wqkv (<class 'torch.nn.modules.linear.Linear'>)\n",
      "Found attention module: model.blocks.12.attn.attn.inner_attn (<class 'flash_attn.modules.mha.SelfAttention'>)\n",
      "Found attention module: model.blocks.12.attn.attn.inner_attn.drop (<class 'torch.nn.modules.dropout.Dropout'>)\n",
      "Found attention module: model.blocks.12.attn.attn.inner_cross_attn (<class 'flash_attn.modules.mha.CrossAttention'>)\n",
      "Found attention module: model.blocks.12.attn.attn.inner_cross_attn.drop (<class 'torch.nn.modules.dropout.Dropout'>)\n",
      "Found attention module: model.blocks.12.attn.attn.out_proj (<class 'torch.nn.modules.linear.Linear'>)\n",
      "Found attention module: model.blocks.13.attn (<class 'satvision_pix4d.models.encoders.models_mae_temporal.FlashMHAWrapper'>)\n",
      "Found attention module: model.blocks.13.attn.attn (<class 'flash_attn.modules.mha.MHA'>)\n",
      "Found attention module: model.blocks.13.attn.attn.Wqkv (<class 'torch.nn.modules.linear.Linear'>)\n",
      "Found attention module: model.blocks.13.attn.attn.inner_attn (<class 'flash_attn.modules.mha.SelfAttention'>)\n",
      "Found attention module: model.blocks.13.attn.attn.inner_attn.drop (<class 'torch.nn.modules.dropout.Dropout'>)\n",
      "Found attention module: model.blocks.13.attn.attn.inner_cross_attn (<class 'flash_attn.modules.mha.CrossAttention'>)\n",
      "Found attention module: model.blocks.13.attn.attn.inner_cross_attn.drop (<class 'torch.nn.modules.dropout.Dropout'>)\n",
      "Found attention module: model.blocks.13.attn.attn.out_proj (<class 'torch.nn.modules.linear.Linear'>)\n",
      "Found attention module: model.blocks.14.attn (<class 'satvision_pix4d.models.encoders.models_mae_temporal.FlashMHAWrapper'>)\n",
      "Found attention module: model.blocks.14.attn.attn (<class 'flash_attn.modules.mha.MHA'>)\n",
      "Found attention module: model.blocks.14.attn.attn.Wqkv (<class 'torch.nn.modules.linear.Linear'>)\n",
      "Found attention module: model.blocks.14.attn.attn.inner_attn (<class 'flash_attn.modules.mha.SelfAttention'>)\n",
      "Found attention module: model.blocks.14.attn.attn.inner_attn.drop (<class 'torch.nn.modules.dropout.Dropout'>)\n",
      "Found attention module: model.blocks.14.attn.attn.inner_cross_attn (<class 'flash_attn.modules.mha.CrossAttention'>)\n",
      "Found attention module: model.blocks.14.attn.attn.inner_cross_attn.drop (<class 'torch.nn.modules.dropout.Dropout'>)\n",
      "Found attention module: model.blocks.14.attn.attn.out_proj (<class 'torch.nn.modules.linear.Linear'>)\n",
      "Found attention module: model.blocks.15.attn (<class 'satvision_pix4d.models.encoders.models_mae_temporal.FlashMHAWrapper'>)\n",
      "Found attention module: model.blocks.15.attn.attn (<class 'flash_attn.modules.mha.MHA'>)\n",
      "Found attention module: model.blocks.15.attn.attn.Wqkv (<class 'torch.nn.modules.linear.Linear'>)\n",
      "Found attention module: model.blocks.15.attn.attn.inner_attn (<class 'flash_attn.modules.mha.SelfAttention'>)\n",
      "Found attention module: model.blocks.15.attn.attn.inner_attn.drop (<class 'torch.nn.modules.dropout.Dropout'>)\n",
      "Found attention module: model.blocks.15.attn.attn.inner_cross_attn (<class 'flash_attn.modules.mha.CrossAttention'>)\n",
      "Found attention module: model.blocks.15.attn.attn.inner_cross_attn.drop (<class 'torch.nn.modules.dropout.Dropout'>)\n",
      "Found attention module: model.blocks.15.attn.attn.out_proj (<class 'torch.nn.modules.linear.Linear'>)\n",
      "Found attention module: model.blocks.16.attn (<class 'satvision_pix4d.models.encoders.models_mae_temporal.FlashMHAWrapper'>)\n",
      "Found attention module: model.blocks.16.attn.attn (<class 'flash_attn.modules.mha.MHA'>)\n",
      "Found attention module: model.blocks.16.attn.attn.Wqkv (<class 'torch.nn.modules.linear.Linear'>)\n",
      "Found attention module: model.blocks.16.attn.attn.inner_attn (<class 'flash_attn.modules.mha.SelfAttention'>)\n",
      "Found attention module: model.blocks.16.attn.attn.inner_attn.drop (<class 'torch.nn.modules.dropout.Dropout'>)\n",
      "Found attention module: model.blocks.16.attn.attn.inner_cross_attn (<class 'flash_attn.modules.mha.CrossAttention'>)\n",
      "Found attention module: model.blocks.16.attn.attn.inner_cross_attn.drop (<class 'torch.nn.modules.dropout.Dropout'>)\n",
      "Found attention module: model.blocks.16.attn.attn.out_proj (<class 'torch.nn.modules.linear.Linear'>)\n",
      "Found attention module: model.blocks.17.attn (<class 'satvision_pix4d.models.encoders.models_mae_temporal.FlashMHAWrapper'>)\n",
      "Found attention module: model.blocks.17.attn.attn (<class 'flash_attn.modules.mha.MHA'>)\n",
      "Found attention module: model.blocks.17.attn.attn.Wqkv (<class 'torch.nn.modules.linear.Linear'>)\n",
      "Found attention module: model.blocks.17.attn.attn.inner_attn (<class 'flash_attn.modules.mha.SelfAttention'>)\n",
      "Found attention module: model.blocks.17.attn.attn.inner_attn.drop (<class 'torch.nn.modules.dropout.Dropout'>)\n",
      "Found attention module: model.blocks.17.attn.attn.inner_cross_attn (<class 'flash_attn.modules.mha.CrossAttention'>)\n",
      "Found attention module: model.blocks.17.attn.attn.inner_cross_attn.drop (<class 'torch.nn.modules.dropout.Dropout'>)\n",
      "Found attention module: model.blocks.17.attn.attn.out_proj (<class 'torch.nn.modules.linear.Linear'>)\n",
      "Found attention module: model.blocks.18.attn (<class 'satvision_pix4d.models.encoders.models_mae_temporal.FlashMHAWrapper'>)\n",
      "Found attention module: model.blocks.18.attn.attn (<class 'flash_attn.modules.mha.MHA'>)\n",
      "Found attention module: model.blocks.18.attn.attn.Wqkv (<class 'torch.nn.modules.linear.Linear'>)\n",
      "Found attention module: model.blocks.18.attn.attn.inner_attn (<class 'flash_attn.modules.mha.SelfAttention'>)\n",
      "Found attention module: model.blocks.18.attn.attn.inner_attn.drop (<class 'torch.nn.modules.dropout.Dropout'>)\n",
      "Found attention module: model.blocks.18.attn.attn.inner_cross_attn (<class 'flash_attn.modules.mha.CrossAttention'>)\n",
      "Found attention module: model.blocks.18.attn.attn.inner_cross_attn.drop (<class 'torch.nn.modules.dropout.Dropout'>)\n",
      "Found attention module: model.blocks.18.attn.attn.out_proj (<class 'torch.nn.modules.linear.Linear'>)\n",
      "Found attention module: model.blocks.19.attn (<class 'satvision_pix4d.models.encoders.models_mae_temporal.FlashMHAWrapper'>)\n",
      "Found attention module: model.blocks.19.attn.attn (<class 'flash_attn.modules.mha.MHA'>)\n",
      "Found attention module: model.blocks.19.attn.attn.Wqkv (<class 'torch.nn.modules.linear.Linear'>)\n",
      "Found attention module: model.blocks.19.attn.attn.inner_attn (<class 'flash_attn.modules.mha.SelfAttention'>)\n",
      "Found attention module: model.blocks.19.attn.attn.inner_attn.drop (<class 'torch.nn.modules.dropout.Dropout'>)\n",
      "Found attention module: model.blocks.19.attn.attn.inner_cross_attn (<class 'flash_attn.modules.mha.CrossAttention'>)\n",
      "Found attention module: model.blocks.19.attn.attn.inner_cross_attn.drop (<class 'torch.nn.modules.dropout.Dropout'>)\n",
      "Found attention module: model.blocks.19.attn.attn.out_proj (<class 'torch.nn.modules.linear.Linear'>)\n",
      "Found attention module: model.blocks.20.attn (<class 'satvision_pix4d.models.encoders.models_mae_temporal.FlashMHAWrapper'>)\n",
      "Found attention module: model.blocks.20.attn.attn (<class 'flash_attn.modules.mha.MHA'>)\n",
      "Found attention module: model.blocks.20.attn.attn.Wqkv (<class 'torch.nn.modules.linear.Linear'>)\n",
      "Found attention module: model.blocks.20.attn.attn.inner_attn (<class 'flash_attn.modules.mha.SelfAttention'>)\n",
      "Found attention module: model.blocks.20.attn.attn.inner_attn.drop (<class 'torch.nn.modules.dropout.Dropout'>)\n",
      "Found attention module: model.blocks.20.attn.attn.inner_cross_attn (<class 'flash_attn.modules.mha.CrossAttention'>)\n",
      "Found attention module: model.blocks.20.attn.attn.inner_cross_attn.drop (<class 'torch.nn.modules.dropout.Dropout'>)\n",
      "Found attention module: model.blocks.20.attn.attn.out_proj (<class 'torch.nn.modules.linear.Linear'>)\n",
      "Found attention module: model.blocks.21.attn (<class 'satvision_pix4d.models.encoders.models_mae_temporal.FlashMHAWrapper'>)\n",
      "Found attention module: model.blocks.21.attn.attn (<class 'flash_attn.modules.mha.MHA'>)\n",
      "Found attention module: model.blocks.21.attn.attn.Wqkv (<class 'torch.nn.modules.linear.Linear'>)\n",
      "Found attention module: model.blocks.21.attn.attn.inner_attn (<class 'flash_attn.modules.mha.SelfAttention'>)\n",
      "Found attention module: model.blocks.21.attn.attn.inner_attn.drop (<class 'torch.nn.modules.dropout.Dropout'>)\n",
      "Found attention module: model.blocks.21.attn.attn.inner_cross_attn (<class 'flash_attn.modules.mha.CrossAttention'>)\n",
      "Found attention module: model.blocks.21.attn.attn.inner_cross_attn.drop (<class 'torch.nn.modules.dropout.Dropout'>)\n",
      "Found attention module: model.blocks.21.attn.attn.out_proj (<class 'torch.nn.modules.linear.Linear'>)\n",
      "Found attention module: model.blocks.22.attn (<class 'satvision_pix4d.models.encoders.models_mae_temporal.FlashMHAWrapper'>)\n",
      "Found attention module: model.blocks.22.attn.attn (<class 'flash_attn.modules.mha.MHA'>)\n",
      "Found attention module: model.blocks.22.attn.attn.Wqkv (<class 'torch.nn.modules.linear.Linear'>)\n",
      "Found attention module: model.blocks.22.attn.attn.inner_attn (<class 'flash_attn.modules.mha.SelfAttention'>)\n",
      "Found attention module: model.blocks.22.attn.attn.inner_attn.drop (<class 'torch.nn.modules.dropout.Dropout'>)\n",
      "Found attention module: model.blocks.22.attn.attn.inner_cross_attn (<class 'flash_attn.modules.mha.CrossAttention'>)\n",
      "Found attention module: model.blocks.22.attn.attn.inner_cross_attn.drop (<class 'torch.nn.modules.dropout.Dropout'>)\n",
      "Found attention module: model.blocks.22.attn.attn.out_proj (<class 'torch.nn.modules.linear.Linear'>)\n",
      "Found attention module: model.blocks.23.attn (<class 'satvision_pix4d.models.encoders.models_mae_temporal.FlashMHAWrapper'>)\n",
      "Found attention module: model.blocks.23.attn.attn (<class 'flash_attn.modules.mha.MHA'>)\n",
      "Found attention module: model.blocks.23.attn.attn.Wqkv (<class 'torch.nn.modules.linear.Linear'>)\n",
      "Found attention module: model.blocks.23.attn.attn.inner_attn (<class 'flash_attn.modules.mha.SelfAttention'>)\n",
      "Found attention module: model.blocks.23.attn.attn.inner_attn.drop (<class 'torch.nn.modules.dropout.Dropout'>)\n",
      "Found attention module: model.blocks.23.attn.attn.inner_cross_attn (<class 'flash_attn.modules.mha.CrossAttention'>)\n",
      "Found attention module: model.blocks.23.attn.attn.inner_cross_attn.drop (<class 'torch.nn.modules.dropout.Dropout'>)\n",
      "Found attention module: model.blocks.23.attn.attn.out_proj (<class 'torch.nn.modules.linear.Linear'>)\n",
      "Found attention module: model.decoder_blocks.0.attn (<class 'satvision_pix4d.models.encoders.models_mae_temporal.FlashMHAWrapper'>)\n",
      "Found attention module: model.decoder_blocks.0.attn.attn (<class 'flash_attn.modules.mha.MHA'>)\n",
      "Found attention module: model.decoder_blocks.0.attn.attn.Wqkv (<class 'torch.nn.modules.linear.Linear'>)\n",
      "Found attention module: model.decoder_blocks.0.attn.attn.inner_attn (<class 'flash_attn.modules.mha.SelfAttention'>)\n",
      "Found attention module: model.decoder_blocks.0.attn.attn.inner_attn.drop (<class 'torch.nn.modules.dropout.Dropout'>)\n",
      "Found attention module: model.decoder_blocks.0.attn.attn.inner_cross_attn (<class 'flash_attn.modules.mha.CrossAttention'>)\n",
      "Found attention module: model.decoder_blocks.0.attn.attn.inner_cross_attn.drop (<class 'torch.nn.modules.dropout.Dropout'>)\n",
      "Found attention module: model.decoder_blocks.0.attn.attn.out_proj (<class 'torch.nn.modules.linear.Linear'>)\n",
      "Found attention module: model.decoder_blocks.1.attn (<class 'satvision_pix4d.models.encoders.models_mae_temporal.FlashMHAWrapper'>)\n",
      "Found attention module: model.decoder_blocks.1.attn.attn (<class 'flash_attn.modules.mha.MHA'>)\n",
      "Found attention module: model.decoder_blocks.1.attn.attn.Wqkv (<class 'torch.nn.modules.linear.Linear'>)\n",
      "Found attention module: model.decoder_blocks.1.attn.attn.inner_attn (<class 'flash_attn.modules.mha.SelfAttention'>)\n",
      "Found attention module: model.decoder_blocks.1.attn.attn.inner_attn.drop (<class 'torch.nn.modules.dropout.Dropout'>)\n",
      "Found attention module: model.decoder_blocks.1.attn.attn.inner_cross_attn (<class 'flash_attn.modules.mha.CrossAttention'>)\n",
      "Found attention module: model.decoder_blocks.1.attn.attn.inner_cross_attn.drop (<class 'torch.nn.modules.dropout.Dropout'>)\n",
      "Found attention module: model.decoder_blocks.1.attn.attn.out_proj (<class 'torch.nn.modules.linear.Linear'>)\n",
      "Found attention module: model.decoder_blocks.2.attn (<class 'satvision_pix4d.models.encoders.models_mae_temporal.FlashMHAWrapper'>)\n",
      "Found attention module: model.decoder_blocks.2.attn.attn (<class 'flash_attn.modules.mha.MHA'>)\n",
      "Found attention module: model.decoder_blocks.2.attn.attn.Wqkv (<class 'torch.nn.modules.linear.Linear'>)\n",
      "Found attention module: model.decoder_blocks.2.attn.attn.inner_attn (<class 'flash_attn.modules.mha.SelfAttention'>)\n",
      "Found attention module: model.decoder_blocks.2.attn.attn.inner_attn.drop (<class 'torch.nn.modules.dropout.Dropout'>)\n",
      "Found attention module: model.decoder_blocks.2.attn.attn.inner_cross_attn (<class 'flash_attn.modules.mha.CrossAttention'>)\n",
      "Found attention module: model.decoder_blocks.2.attn.attn.inner_cross_attn.drop (<class 'torch.nn.modules.dropout.Dropout'>)\n",
      "Found attention module: model.decoder_blocks.2.attn.attn.out_proj (<class 'torch.nn.modules.linear.Linear'>)\n",
      "Found attention module: model.decoder_blocks.3.attn (<class 'satvision_pix4d.models.encoders.models_mae_temporal.FlashMHAWrapper'>)\n",
      "Found attention module: model.decoder_blocks.3.attn.attn (<class 'flash_attn.modules.mha.MHA'>)\n",
      "Found attention module: model.decoder_blocks.3.attn.attn.Wqkv (<class 'torch.nn.modules.linear.Linear'>)\n",
      "Found attention module: model.decoder_blocks.3.attn.attn.inner_attn (<class 'flash_attn.modules.mha.SelfAttention'>)\n",
      "Found attention module: model.decoder_blocks.3.attn.attn.inner_attn.drop (<class 'torch.nn.modules.dropout.Dropout'>)\n",
      "Found attention module: model.decoder_blocks.3.attn.attn.inner_cross_attn (<class 'flash_attn.modules.mha.CrossAttention'>)\n",
      "Found attention module: model.decoder_blocks.3.attn.attn.inner_cross_attn.drop (<class 'torch.nn.modules.dropout.Dropout'>)\n",
      "Found attention module: model.decoder_blocks.3.attn.attn.out_proj (<class 'torch.nn.modules.linear.Linear'>)\n",
      "Found attention module: model.decoder_blocks.4.attn (<class 'satvision_pix4d.models.encoders.models_mae_temporal.FlashMHAWrapper'>)\n",
      "Found attention module: model.decoder_blocks.4.attn.attn (<class 'flash_attn.modules.mha.MHA'>)\n",
      "Found attention module: model.decoder_blocks.4.attn.attn.Wqkv (<class 'torch.nn.modules.linear.Linear'>)\n",
      "Found attention module: model.decoder_blocks.4.attn.attn.inner_attn (<class 'flash_attn.modules.mha.SelfAttention'>)\n",
      "Found attention module: model.decoder_blocks.4.attn.attn.inner_attn.drop (<class 'torch.nn.modules.dropout.Dropout'>)\n",
      "Found attention module: model.decoder_blocks.4.attn.attn.inner_cross_attn (<class 'flash_attn.modules.mha.CrossAttention'>)\n",
      "Found attention module: model.decoder_blocks.4.attn.attn.inner_cross_attn.drop (<class 'torch.nn.modules.dropout.Dropout'>)\n",
      "Found attention module: model.decoder_blocks.4.attn.attn.out_proj (<class 'torch.nn.modules.linear.Linear'>)\n",
      "Found attention module: model.decoder_blocks.5.attn (<class 'satvision_pix4d.models.encoders.models_mae_temporal.FlashMHAWrapper'>)\n",
      "Found attention module: model.decoder_blocks.5.attn.attn (<class 'flash_attn.modules.mha.MHA'>)\n",
      "Found attention module: model.decoder_blocks.5.attn.attn.Wqkv (<class 'torch.nn.modules.linear.Linear'>)\n",
      "Found attention module: model.decoder_blocks.5.attn.attn.inner_attn (<class 'flash_attn.modules.mha.SelfAttention'>)\n",
      "Found attention module: model.decoder_blocks.5.attn.attn.inner_attn.drop (<class 'torch.nn.modules.dropout.Dropout'>)\n",
      "Found attention module: model.decoder_blocks.5.attn.attn.inner_cross_attn (<class 'flash_attn.modules.mha.CrossAttention'>)\n",
      "Found attention module: model.decoder_blocks.5.attn.attn.inner_cross_attn.drop (<class 'torch.nn.modules.dropout.Dropout'>)\n",
      "Found attention module: model.decoder_blocks.5.attn.attn.out_proj (<class 'torch.nn.modules.linear.Linear'>)\n",
      "Found attention module: model.decoder_blocks.6.attn (<class 'satvision_pix4d.models.encoders.models_mae_temporal.FlashMHAWrapper'>)\n",
      "Found attention module: model.decoder_blocks.6.attn.attn (<class 'flash_attn.modules.mha.MHA'>)\n",
      "Found attention module: model.decoder_blocks.6.attn.attn.Wqkv (<class 'torch.nn.modules.linear.Linear'>)\n",
      "Found attention module: model.decoder_blocks.6.attn.attn.inner_attn (<class 'flash_attn.modules.mha.SelfAttention'>)\n",
      "Found attention module: model.decoder_blocks.6.attn.attn.inner_attn.drop (<class 'torch.nn.modules.dropout.Dropout'>)\n",
      "Found attention module: model.decoder_blocks.6.attn.attn.inner_cross_attn (<class 'flash_attn.modules.mha.CrossAttention'>)\n",
      "Found attention module: model.decoder_blocks.6.attn.attn.inner_cross_attn.drop (<class 'torch.nn.modules.dropout.Dropout'>)\n",
      "Found attention module: model.decoder_blocks.6.attn.attn.out_proj (<class 'torch.nn.modules.linear.Linear'>)\n",
      "Found attention module: model.decoder_blocks.7.attn (<class 'satvision_pix4d.models.encoders.models_mae_temporal.FlashMHAWrapper'>)\n",
      "Found attention module: model.decoder_blocks.7.attn.attn (<class 'flash_attn.modules.mha.MHA'>)\n",
      "Found attention module: model.decoder_blocks.7.attn.attn.Wqkv (<class 'torch.nn.modules.linear.Linear'>)\n",
      "Found attention module: model.decoder_blocks.7.attn.attn.inner_attn (<class 'flash_attn.modules.mha.SelfAttention'>)\n",
      "Found attention module: model.decoder_blocks.7.attn.attn.inner_attn.drop (<class 'torch.nn.modules.dropout.Dropout'>)\n",
      "Found attention module: model.decoder_blocks.7.attn.attn.inner_cross_attn (<class 'flash_attn.modules.mha.CrossAttention'>)\n",
      "Found attention module: model.decoder_blocks.7.attn.attn.inner_cross_attn.drop (<class 'torch.nn.modules.dropout.Dropout'>)\n",
      "Found attention module: model.decoder_blocks.7.attn.attn.out_proj (<class 'torch.nn.modules.linear.Linear'>)\n",
      "Registered hook on: model.blocks.0.attn\n",
      "Registered hook on: model.blocks.0.attn.attn\n",
      "Registered hook on: model.blocks.0.attn.attn.Wqkv\n",
      "Registered hook on: model.blocks.0.attn.attn.inner_attn\n",
      "Registered hook on: model.blocks.0.attn.attn.inner_attn.drop\n",
      "Registered hook on: model.blocks.0.attn.attn.inner_cross_attn\n",
      "Registered hook on: model.blocks.0.attn.attn.inner_cross_attn.drop\n",
      "Registered hook on: model.blocks.0.attn.attn.out_proj\n",
      "Registered hook on: model.blocks.1.attn\n",
      "Registered hook on: model.blocks.1.attn.attn\n",
      "Registered hook on: model.blocks.1.attn.attn.Wqkv\n",
      "Registered hook on: model.blocks.1.attn.attn.inner_attn\n",
      "Registered hook on: model.blocks.1.attn.attn.inner_attn.drop\n",
      "Registered hook on: model.blocks.1.attn.attn.inner_cross_attn\n",
      "Registered hook on: model.blocks.1.attn.attn.inner_cross_attn.drop\n",
      "Registered hook on: model.blocks.1.attn.attn.out_proj\n",
      "Registered hook on: model.blocks.2.attn\n",
      "Registered hook on: model.blocks.2.attn.attn\n",
      "Registered hook on: model.blocks.2.attn.attn.Wqkv\n",
      "Registered hook on: model.blocks.2.attn.attn.inner_attn\n",
      "Registered hook on: model.blocks.2.attn.attn.inner_attn.drop\n",
      "Registered hook on: model.blocks.2.attn.attn.inner_cross_attn\n",
      "Registered hook on: model.blocks.2.attn.attn.inner_cross_attn.drop\n",
      "Registered hook on: model.blocks.2.attn.attn.out_proj\n",
      "Registered hook on: model.blocks.3.attn\n",
      "Registered hook on: model.blocks.3.attn.attn\n",
      "Registered hook on: model.blocks.3.attn.attn.Wqkv\n",
      "Registered hook on: model.blocks.3.attn.attn.inner_attn\n",
      "Registered hook on: model.blocks.3.attn.attn.inner_attn.drop\n",
      "Registered hook on: model.blocks.3.attn.attn.inner_cross_attn\n",
      "Registered hook on: model.blocks.3.attn.attn.inner_cross_attn.drop\n",
      "Registered hook on: model.blocks.3.attn.attn.out_proj\n",
      "Registered hook on: model.blocks.4.attn\n",
      "Registered hook on: model.blocks.4.attn.attn\n",
      "Registered hook on: model.blocks.4.attn.attn.Wqkv\n",
      "Registered hook on: model.blocks.4.attn.attn.inner_attn\n",
      "Registered hook on: model.blocks.4.attn.attn.inner_attn.drop\n",
      "Registered hook on: model.blocks.4.attn.attn.inner_cross_attn\n",
      "Registered hook on: model.blocks.4.attn.attn.inner_cross_attn.drop\n",
      "Registered hook on: model.blocks.4.attn.attn.out_proj\n",
      "Registered hook on: model.blocks.5.attn\n",
      "Registered hook on: model.blocks.5.attn.attn\n",
      "Registered hook on: model.blocks.5.attn.attn.Wqkv\n",
      "Registered hook on: model.blocks.5.attn.attn.inner_attn\n",
      "Registered hook on: model.blocks.5.attn.attn.inner_attn.drop\n",
      "Registered hook on: model.blocks.5.attn.attn.inner_cross_attn\n",
      "Registered hook on: model.blocks.5.attn.attn.inner_cross_attn.drop\n",
      "Registered hook on: model.blocks.5.attn.attn.out_proj\n",
      "Registered hook on: model.blocks.6.attn\n",
      "Registered hook on: model.blocks.6.attn.attn\n",
      "Registered hook on: model.blocks.6.attn.attn.Wqkv\n",
      "Registered hook on: model.blocks.6.attn.attn.inner_attn\n",
      "Registered hook on: model.blocks.6.attn.attn.inner_attn.drop\n",
      "Registered hook on: model.blocks.6.attn.attn.inner_cross_attn\n",
      "Registered hook on: model.blocks.6.attn.attn.inner_cross_attn.drop\n",
      "Registered hook on: model.blocks.6.attn.attn.out_proj\n",
      "Registered hook on: model.blocks.7.attn\n",
      "Registered hook on: model.blocks.7.attn.attn\n",
      "Registered hook on: model.blocks.7.attn.attn.Wqkv\n",
      "Registered hook on: model.blocks.7.attn.attn.inner_attn\n",
      "Registered hook on: model.blocks.7.attn.attn.inner_attn.drop\n",
      "Registered hook on: model.blocks.7.attn.attn.inner_cross_attn\n",
      "Registered hook on: model.blocks.7.attn.attn.inner_cross_attn.drop\n",
      "Registered hook on: model.blocks.7.attn.attn.out_proj\n",
      "Registered hook on: model.blocks.8.attn\n",
      "Registered hook on: model.blocks.8.attn.attn\n",
      "Registered hook on: model.blocks.8.attn.attn.Wqkv\n",
      "Registered hook on: model.blocks.8.attn.attn.inner_attn\n",
      "Registered hook on: model.blocks.8.attn.attn.inner_attn.drop\n",
      "Registered hook on: model.blocks.8.attn.attn.inner_cross_attn\n",
      "Registered hook on: model.blocks.8.attn.attn.inner_cross_attn.drop\n",
      "Registered hook on: model.blocks.8.attn.attn.out_proj\n",
      "Registered hook on: model.blocks.9.attn\n",
      "Registered hook on: model.blocks.9.attn.attn\n",
      "Registered hook on: model.blocks.9.attn.attn.Wqkv\n",
      "Registered hook on: model.blocks.9.attn.attn.inner_attn\n",
      "Registered hook on: model.blocks.9.attn.attn.inner_attn.drop\n",
      "Registered hook on: model.blocks.9.attn.attn.inner_cross_attn\n",
      "Registered hook on: model.blocks.9.attn.attn.inner_cross_attn.drop\n",
      "Registered hook on: model.blocks.9.attn.attn.out_proj\n",
      "Registered hook on: model.blocks.10.attn\n",
      "Registered hook on: model.blocks.10.attn.attn\n",
      "Registered hook on: model.blocks.10.attn.attn.Wqkv\n",
      "Registered hook on: model.blocks.10.attn.attn.inner_attn\n",
      "Registered hook on: model.blocks.10.attn.attn.inner_attn.drop\n",
      "Registered hook on: model.blocks.10.attn.attn.inner_cross_attn\n",
      "Registered hook on: model.blocks.10.attn.attn.inner_cross_attn.drop\n",
      "Registered hook on: model.blocks.10.attn.attn.out_proj\n",
      "Registered hook on: model.blocks.11.attn\n",
      "Registered hook on: model.blocks.11.attn.attn\n",
      "Registered hook on: model.blocks.11.attn.attn.Wqkv\n",
      "Registered hook on: model.blocks.11.attn.attn.inner_attn\n",
      "Registered hook on: model.blocks.11.attn.attn.inner_attn.drop\n",
      "Registered hook on: model.blocks.11.attn.attn.inner_cross_attn\n",
      "Registered hook on: model.blocks.11.attn.attn.inner_cross_attn.drop\n",
      "Registered hook on: model.blocks.11.attn.attn.out_proj\n",
      "Registered hook on: model.blocks.12.attn\n",
      "Registered hook on: model.blocks.12.attn.attn\n",
      "Registered hook on: model.blocks.12.attn.attn.Wqkv\n",
      "Registered hook on: model.blocks.12.attn.attn.inner_attn\n",
      "Registered hook on: model.blocks.12.attn.attn.inner_attn.drop\n",
      "Registered hook on: model.blocks.12.attn.attn.inner_cross_attn\n",
      "Registered hook on: model.blocks.12.attn.attn.inner_cross_attn.drop\n",
      "Registered hook on: model.blocks.12.attn.attn.out_proj\n",
      "Registered hook on: model.blocks.13.attn\n",
      "Registered hook on: model.blocks.13.attn.attn\n",
      "Registered hook on: model.blocks.13.attn.attn.Wqkv\n",
      "Registered hook on: model.blocks.13.attn.attn.inner_attn\n",
      "Registered hook on: model.blocks.13.attn.attn.inner_attn.drop\n",
      "Registered hook on: model.blocks.13.attn.attn.inner_cross_attn\n",
      "Registered hook on: model.blocks.13.attn.attn.inner_cross_attn.drop\n",
      "Registered hook on: model.blocks.13.attn.attn.out_proj\n",
      "Registered hook on: model.blocks.14.attn\n",
      "Registered hook on: model.blocks.14.attn.attn\n",
      "Registered hook on: model.blocks.14.attn.attn.Wqkv\n",
      "Registered hook on: model.blocks.14.attn.attn.inner_attn\n",
      "Registered hook on: model.blocks.14.attn.attn.inner_attn.drop\n",
      "Registered hook on: model.blocks.14.attn.attn.inner_cross_attn\n",
      "Registered hook on: model.blocks.14.attn.attn.inner_cross_attn.drop\n",
      "Registered hook on: model.blocks.14.attn.attn.out_proj\n",
      "Registered hook on: model.blocks.15.attn\n",
      "Registered hook on: model.blocks.15.attn.attn\n",
      "Registered hook on: model.blocks.15.attn.attn.Wqkv\n",
      "Registered hook on: model.blocks.15.attn.attn.inner_attn\n",
      "Registered hook on: model.blocks.15.attn.attn.inner_attn.drop\n",
      "Registered hook on: model.blocks.15.attn.attn.inner_cross_attn\n",
      "Registered hook on: model.blocks.15.attn.attn.inner_cross_attn.drop\n",
      "Registered hook on: model.blocks.15.attn.attn.out_proj\n",
      "Registered hook on: model.blocks.16.attn\n",
      "Registered hook on: model.blocks.16.attn.attn\n",
      "Registered hook on: model.blocks.16.attn.attn.Wqkv\n",
      "Registered hook on: model.blocks.16.attn.attn.inner_attn\n",
      "Registered hook on: model.blocks.16.attn.attn.inner_attn.drop\n",
      "Registered hook on: model.blocks.16.attn.attn.inner_cross_attn\n",
      "Registered hook on: model.blocks.16.attn.attn.inner_cross_attn.drop\n",
      "Registered hook on: model.blocks.16.attn.attn.out_proj\n",
      "Registered hook on: model.blocks.17.attn\n",
      "Registered hook on: model.blocks.17.attn.attn\n",
      "Registered hook on: model.blocks.17.attn.attn.Wqkv\n",
      "Registered hook on: model.blocks.17.attn.attn.inner_attn\n",
      "Registered hook on: model.blocks.17.attn.attn.inner_attn.drop\n",
      "Registered hook on: model.blocks.17.attn.attn.inner_cross_attn\n",
      "Registered hook on: model.blocks.17.attn.attn.inner_cross_attn.drop\n",
      "Registered hook on: model.blocks.17.attn.attn.out_proj\n",
      "Registered hook on: model.blocks.18.attn\n",
      "Registered hook on: model.blocks.18.attn.attn\n",
      "Registered hook on: model.blocks.18.attn.attn.Wqkv\n",
      "Registered hook on: model.blocks.18.attn.attn.inner_attn\n",
      "Registered hook on: model.blocks.18.attn.attn.inner_attn.drop\n",
      "Registered hook on: model.blocks.18.attn.attn.inner_cross_attn\n",
      "Registered hook on: model.blocks.18.attn.attn.inner_cross_attn.drop\n",
      "Registered hook on: model.blocks.18.attn.attn.out_proj\n",
      "Registered hook on: model.blocks.19.attn\n",
      "Registered hook on: model.blocks.19.attn.attn\n",
      "Registered hook on: model.blocks.19.attn.attn.Wqkv\n",
      "Registered hook on: model.blocks.19.attn.attn.inner_attn\n",
      "Registered hook on: model.blocks.19.attn.attn.inner_attn.drop\n",
      "Registered hook on: model.blocks.19.attn.attn.inner_cross_attn\n",
      "Registered hook on: model.blocks.19.attn.attn.inner_cross_attn.drop\n",
      "Registered hook on: model.blocks.19.attn.attn.out_proj\n",
      "Registered hook on: model.blocks.20.attn\n",
      "Registered hook on: model.blocks.20.attn.attn\n",
      "Registered hook on: model.blocks.20.attn.attn.Wqkv\n",
      "Registered hook on: model.blocks.20.attn.attn.inner_attn\n",
      "Registered hook on: model.blocks.20.attn.attn.inner_attn.drop\n",
      "Registered hook on: model.blocks.20.attn.attn.inner_cross_attn\n",
      "Registered hook on: model.blocks.20.attn.attn.inner_cross_attn.drop\n",
      "Registered hook on: model.blocks.20.attn.attn.out_proj\n",
      "Registered hook on: model.blocks.21.attn\n",
      "Registered hook on: model.blocks.21.attn.attn\n",
      "Registered hook on: model.blocks.21.attn.attn.Wqkv\n",
      "Registered hook on: model.blocks.21.attn.attn.inner_attn\n",
      "Registered hook on: model.blocks.21.attn.attn.inner_attn.drop\n",
      "Registered hook on: model.blocks.21.attn.attn.inner_cross_attn\n",
      "Registered hook on: model.blocks.21.attn.attn.inner_cross_attn.drop\n",
      "Registered hook on: model.blocks.21.attn.attn.out_proj\n",
      "Registered hook on: model.blocks.22.attn\n",
      "Registered hook on: model.blocks.22.attn.attn\n",
      "Registered hook on: model.blocks.22.attn.attn.Wqkv\n",
      "Registered hook on: model.blocks.22.attn.attn.inner_attn\n",
      "Registered hook on: model.blocks.22.attn.attn.inner_attn.drop\n",
      "Registered hook on: model.blocks.22.attn.attn.inner_cross_attn\n",
      "Registered hook on: model.blocks.22.attn.attn.inner_cross_attn.drop\n",
      "Registered hook on: model.blocks.22.attn.attn.out_proj\n",
      "Registered hook on: model.blocks.23.attn\n",
      "Registered hook on: model.blocks.23.attn.attn\n",
      "Registered hook on: model.blocks.23.attn.attn.Wqkv\n",
      "Registered hook on: model.blocks.23.attn.attn.inner_attn\n",
      "Registered hook on: model.blocks.23.attn.attn.inner_attn.drop\n",
      "Registered hook on: model.blocks.23.attn.attn.inner_cross_attn\n",
      "Registered hook on: model.blocks.23.attn.attn.inner_cross_attn.drop\n",
      "Registered hook on: model.blocks.23.attn.attn.out_proj\n",
      "Registered hook on: model.decoder_blocks.0.attn\n",
      "Registered hook on: model.decoder_blocks.0.attn.attn\n",
      "Registered hook on: model.decoder_blocks.0.attn.attn.Wqkv\n",
      "Registered hook on: model.decoder_blocks.0.attn.attn.inner_attn\n",
      "Registered hook on: model.decoder_blocks.0.attn.attn.inner_attn.drop\n",
      "Registered hook on: model.decoder_blocks.0.attn.attn.inner_cross_attn\n",
      "Registered hook on: model.decoder_blocks.0.attn.attn.inner_cross_attn.drop\n",
      "Registered hook on: model.decoder_blocks.0.attn.attn.out_proj\n",
      "Registered hook on: model.decoder_blocks.1.attn\n",
      "Registered hook on: model.decoder_blocks.1.attn.attn\n",
      "Registered hook on: model.decoder_blocks.1.attn.attn.Wqkv\n",
      "Registered hook on: model.decoder_blocks.1.attn.attn.inner_attn\n",
      "Registered hook on: model.decoder_blocks.1.attn.attn.inner_attn.drop\n",
      "Registered hook on: model.decoder_blocks.1.attn.attn.inner_cross_attn\n",
      "Registered hook on: model.decoder_blocks.1.attn.attn.inner_cross_attn.drop\n",
      "Registered hook on: model.decoder_blocks.1.attn.attn.out_proj\n",
      "Registered hook on: model.decoder_blocks.2.attn\n",
      "Registered hook on: model.decoder_blocks.2.attn.attn\n",
      "Registered hook on: model.decoder_blocks.2.attn.attn.Wqkv\n",
      "Registered hook on: model.decoder_blocks.2.attn.attn.inner_attn\n",
      "Registered hook on: model.decoder_blocks.2.attn.attn.inner_attn.drop\n",
      "Registered hook on: model.decoder_blocks.2.attn.attn.inner_cross_attn\n",
      "Registered hook on: model.decoder_blocks.2.attn.attn.inner_cross_attn.drop\n",
      "Registered hook on: model.decoder_blocks.2.attn.attn.out_proj\n",
      "Registered hook on: model.decoder_blocks.3.attn\n",
      "Registered hook on: model.decoder_blocks.3.attn.attn\n",
      "Registered hook on: model.decoder_blocks.3.attn.attn.Wqkv\n",
      "Registered hook on: model.decoder_blocks.3.attn.attn.inner_attn\n",
      "Registered hook on: model.decoder_blocks.3.attn.attn.inner_attn.drop\n",
      "Registered hook on: model.decoder_blocks.3.attn.attn.inner_cross_attn\n",
      "Registered hook on: model.decoder_blocks.3.attn.attn.inner_cross_attn.drop\n",
      "Registered hook on: model.decoder_blocks.3.attn.attn.out_proj\n",
      "Registered hook on: model.decoder_blocks.4.attn\n",
      "Registered hook on: model.decoder_blocks.4.attn.attn\n",
      "Registered hook on: model.decoder_blocks.4.attn.attn.Wqkv\n",
      "Registered hook on: model.decoder_blocks.4.attn.attn.inner_attn\n",
      "Registered hook on: model.decoder_blocks.4.attn.attn.inner_attn.drop\n",
      "Registered hook on: model.decoder_blocks.4.attn.attn.inner_cross_attn\n",
      "Registered hook on: model.decoder_blocks.4.attn.attn.inner_cross_attn.drop\n",
      "Registered hook on: model.decoder_blocks.4.attn.attn.out_proj\n",
      "Registered hook on: model.decoder_blocks.5.attn\n",
      "Registered hook on: model.decoder_blocks.5.attn.attn\n",
      "Registered hook on: model.decoder_blocks.5.attn.attn.Wqkv\n",
      "Registered hook on: model.decoder_blocks.5.attn.attn.inner_attn\n",
      "Registered hook on: model.decoder_blocks.5.attn.attn.inner_attn.drop\n",
      "Registered hook on: model.decoder_blocks.5.attn.attn.inner_cross_attn\n",
      "Registered hook on: model.decoder_blocks.5.attn.attn.inner_cross_attn.drop\n",
      "Registered hook on: model.decoder_blocks.5.attn.attn.out_proj\n",
      "Registered hook on: model.decoder_blocks.6.attn\n",
      "Registered hook on: model.decoder_blocks.6.attn.attn\n",
      "Registered hook on: model.decoder_blocks.6.attn.attn.Wqkv\n",
      "Registered hook on: model.decoder_blocks.6.attn.attn.inner_attn\n",
      "Registered hook on: model.decoder_blocks.6.attn.attn.inner_attn.drop\n",
      "Registered hook on: model.decoder_blocks.6.attn.attn.inner_cross_attn\n",
      "Registered hook on: model.decoder_blocks.6.attn.attn.inner_cross_attn.drop\n",
      "Registered hook on: model.decoder_blocks.6.attn.attn.out_proj\n",
      "Registered hook on: model.decoder_blocks.7.attn\n",
      "Registered hook on: model.decoder_blocks.7.attn.attn\n",
      "Registered hook on: model.decoder_blocks.7.attn.attn.Wqkv\n",
      "Registered hook on: model.decoder_blocks.7.attn.attn.inner_attn\n",
      "Registered hook on: model.decoder_blocks.7.attn.attn.inner_attn.drop\n",
      "Registered hook on: model.decoder_blocks.7.attn.attn.inner_cross_attn\n",
      "Registered hook on: model.decoder_blocks.7.attn.attn.inner_cross_attn.drop\n",
      "Registered hook on: model.decoder_blocks.7.attn.attn.out_proj\n",
      "Registered 256 hooks\n",
      "Captured attention from direct output: torch.Size([1, 16, 2458, 2458])\n",
      "Captured attention from direct output: torch.Size([1, 16, 2458, 2458])\n",
      "Captured attention from direct output: torch.Size([1, 16, 2458, 2458])\n",
      "Captured attention from direct output: torch.Size([1, 16, 2458, 2458])\n",
      "Captured attention from direct output: torch.Size([1, 16, 2458, 2458])\n",
      "Captured attention from direct output: torch.Size([1, 16, 2458, 2458])\n",
      "Captured attention from direct output: torch.Size([1, 16, 2458, 2458])\n",
      "Captured attention from direct output: torch.Size([1, 16, 2458, 2458])\n",
      "Captured attention from direct output: torch.Size([1, 16, 2458, 2458])\n",
      "Captured attention from direct output: torch.Size([1, 16, 2458, 2458])\n"
     ]
    }
   ],
   "source": [
    "# COMPLETE ATTENTION EXTRACTION CODE\n",
    "\n",
    "import sys\n",
    "sys.path.append('/explore/nobackup/people/jacaraba/development/satvision-pix4d')\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import inspect\n",
    "from scipy import ndimage\n",
    "\n",
    "from satvision_pix4d.configs.config import _C, _update_config_from_file\n",
    "from satvision_pix4d.pipelines import PIPELINES\n",
    "from satvision_pix4d.datasets.abi_temporal_dataset import ABITemporalDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# STEP 1: LOAD MODEL\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"STEP 1: LOADING MODEL\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "model_filename = '/explore/nobackup/projects/pix4dcloud/jacaraba/model_development/satmae/satmae_satvision_pix4d_pretrain-dev/satmae_satvision_pix4d_pretrain-dev/best-epoch=196-val_loss=47.0728.ckpt/checkpoint/mp_rank_00_model_states.pt'\n",
    "config_filename = '/explore/nobackup/people/jacaraba/development/satvision-pix4d/tests/configs/test_satmae_dev.yaml'\n",
    "\n",
    "config = _C.clone()\n",
    "_update_config_from_file(config, config_filename)\n",
    "print(\"Loaded configuration file.\")\n",
    "\n",
    "config.defrost()\n",
    "config.MODEL.PRETRAINED = model_filename\n",
    "config.OUTPUT = '.'\n",
    "config.freeze()\n",
    "print(\"Updated configuration file.\")\n",
    "\n",
    "pipeline = PIPELINES[config.PIPELINE]\n",
    "ptlPipeline = pipeline(config)\n",
    "\n",
    "checkpoint_data = torch.load(config.MODEL.PRETRAINED, map_location='cpu', weights_only=False)\n",
    "model = ptlPipeline.load_checkpoint(config.MODEL.PRETRAINED, config)\n",
    "model.cpu()\n",
    "model.eval()\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "print(f\"Model type: {type(model)}\")\n",
    "\n",
    "sig = inspect.signature(model.forward)\n",
    "print(f\"Model forward parameters: {list(sig.parameters.keys())}\")\n",
    "\n",
    "# STEP 2: LOAD DATA\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"STEP 2: LOADING DATA\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "data_dir = '/explore/nobackup/people/jacaraba/projects/SatVision-Pix4d/tiles_pix4d/'\n",
    "\n",
    "dataset = ABITemporalDataset(\n",
    "    data_paths=[data_dir], \n",
    "    img_size=512,\n",
    "    in_chans=16,\n",
    "    data_var='Rad'\n",
    ")\n",
    "\n",
    "print(f\"Dataset created with {len(dataset)} samples\")\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "for batch in dataloader:\n",
    "    imgs = batch\n",
    "    break\n",
    "\n",
    "print(f\"Data loaded successfully!\")\n",
    "print(f\"imgs type: {type(imgs)}\")\n",
    "\n",
    "if isinstance(imgs, list):\n",
    "    print(f\"imgs is a list with {len(imgs)} elements\")\n",
    "    if len(imgs) > 0:\n",
    "        if isinstance(imgs[0], torch.Tensor):\n",
    "            try:\n",
    "                imgs = torch.stack(imgs)\n",
    "                print(f\"Converted list to tensor: {imgs.shape}\")\n",
    "            except:\n",
    "                imgs = imgs[0]\n",
    "                print(f\"Using first tensor from list: {imgs.shape}\")\n",
    "        else:\n",
    "            print(f\"First element type: {type(imgs[0])}\")\n",
    "            try:\n",
    "                imgs = torch.tensor(imgs)\n",
    "                print(f\"Converted to tensor: {imgs.shape}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Could not convert to tensor: {e}\")\n",
    "                imgs = None\n",
    "elif isinstance(imgs, torch.Tensor):\n",
    "    print(f\"imgs is already a tensor: {imgs.shape}\")\n",
    "else:\n",
    "    print(f\"Unexpected imgs type: {type(imgs)}\")\n",
    "    try:\n",
    "        imgs = torch.tensor(imgs)\n",
    "        print(f\"Forced conversion to tensor: {imgs.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not convert to tensor: {e}\")\n",
    "        imgs = None\n",
    "\n",
    "if imgs is not None:\n",
    "    print(f\"Final imgs shape: {imgs.shape}\")\n",
    "    print(f\"Final imgs type: {type(imgs)}\")\n",
    "else:\n",
    "    print(\"Failed to load imgs as tensor\")\n",
    "    \n",
    "# STEP 3: CREATE TIMESTAMPS AND RUN INFERENCE\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"STEP 3: CREATING TIMESTAMPS AND RUNNING INFERENCE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if imgs is None:\n",
    "    print(\"Cannot create timestamps - imgs is None\")\n",
    "    timestamps = None\n",
    "    pred_imgs = None\n",
    "else:\n",
    "    batch_size, time_steps = imgs.shape[0], imgs.shape[1]\n",
    "    print(f\"Batch size: {batch_size}, Time steps: {time_steps}\")\n",
    "\n",
    "    timestamps = torch.arange(time_steps).unsqueeze(0).expand(batch_size, -1)\n",
    "    print(f\"timestamps shape: {timestamps.shape}\")\n",
    "    print(f\"timestamps: {timestamps}\")\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            print(\"Attempting model inference...\")\n",
    "            model_output = model(samples=imgs, timestamps=timestamps)\n",
    "            \n",
    "            if isinstance(model_output, tuple):\n",
    "                print(f\"Model returned tuple with {len(model_output)} elements\")\n",
    "                for i, output in enumerate(model_output):\n",
    "                    if hasattr(output, 'shape'):\n",
    "                        print(f\"  Output {i}: {output.shape}\")\n",
    "                    else:\n",
    "                        print(f\"  Output {i}: {type(output)}\")\n",
    "                \n",
    "                pred_imgs = model_output[0]\n",
    "                print(f\"Using first element as pred_imgs: {pred_imgs.shape}\")\n",
    "            else:\n",
    "                pred_imgs = model_output\n",
    "                print(f\"Model inference successful!\")\n",
    "                print(f\"pred_imgs shape: {pred_imgs.shape}\")\n",
    "                \n",
    "            print(f\"pred_imgs type: {type(pred_imgs)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Model inference failed: {e}\")\n",
    "            print(f\"Error type: {type(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            pred_imgs = None\n",
    "            \n",
    "# STEP 4: ATTENTION EXTRACTION FUNCTIONS\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"STEP 4: DEFINING ATTENTION EXTRACTION FUNCTIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def get_last_self_attention(model, samples, timestamps, return_attn=True):\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        if hasattr(model, 'model') and hasattr(model.model, 'forward_encoder'):\n",
    "            try:\n",
    "                sig = inspect.signature(model.model.forward_encoder)\n",
    "                if 'return_attn' in sig.parameters:\n",
    "                    encoded, attn_weights = model.model.forward_encoder(samples, timestamps, return_attn=True)\n",
    "                    print(f\"Got attention from forward_encoder: {attn_weights.shape}\")\n",
    "                    return attn_weights\n",
    "            except Exception as e:\n",
    "                print(f\"forward_encoder approach failed: {e}\")\n",
    "        \n",
    "        print(\"Searching for attention modules...\")\n",
    "        attention_modules = []\n",
    "        for name, module in model.named_modules():\n",
    "            module_name_lower = name.lower()\n",
    "            module_type = str(type(module)).lower()\n",
    "            \n",
    "            if any(keyword in module_name_lower for keyword in ['attn', 'attention', 'self_attn']):\n",
    "                attention_modules.append((name, module))\n",
    "                print(f\"Found attention module: {name} ({type(module)})\")\n",
    "            elif any(keyword in module_type for keyword in ['attention', 'multihead']):\n",
    "                attention_modules.append((name, module))\n",
    "                print(f\"Found attention module by type: {name} ({type(module)})\")\n",
    "        \n",
    "        if not attention_modules:\n",
    "            print(\"No attention modules found\")\n",
    "            return None\n",
    "        \n",
    "        attn_weights = []\n",
    "        \n",
    "        def attention_hook(module, input, output):\n",
    "            if isinstance(output, tuple):\n",
    "                for i, out in enumerate(output):\n",
    "                    if hasattr(out, 'shape') and len(out.shape) == 4:\n",
    "                        if out.shape[-1] == out.shape[-2] and out.shape[-1] > 1:\n",
    "                            print(f\"Captured attention from tuple element {i}: {out.shape}\")\n",
    "                            attn_weights.append(out)\n",
    "            elif hasattr(output, 'shape') and len(output.shape) == 4:\n",
    "                if output.shape[-1] == output.shape[-2] and output.shape[-1] > 1:\n",
    "                    print(f\"Captured attention from direct output: {output.shape}\")\n",
    "                    attn_weights.append(output)\n",
    "            \n",
    "            if hasattr(module, 'attn_weights'):\n",
    "                print(f\"Found stored attention weights: {module.attn_weights.shape}\")\n",
    "                attn_weights.append(module.attn_weights)\n",
    "        \n",
    "        hooks = []\n",
    "        for name, module in attention_modules:\n",
    "            hook = module.register_forward_hook(attention_hook)\n",
    "            hooks.append(hook)\n",
    "            print(f\"Registered hook on: {name}\")\n",
    "        \n",
    "        print(f\"Registered {len(hooks)} hooks\")\n",
    "        \n",
    "        try:\n",
    "            output = model(samples=samples, timestamps=timestamps)\n",
    "            print(\"Forward pass completed for attention extraction\")\n",
    "        except Exception as e:\n",
    "            print(f\"Forward pass failed during attention extraction: {e}\")\n",
    "        \n",
    "        for hook in hooks:\n",
    "            hook.remove()\n",
    "        \n",
    "        if attn_weights:\n",
    "            print(f\"Found {len(attn_weights)} attention tensors\")\n",
    "            for i, attn in enumerate(attn_weights):\n",
    "                print(f\"  Attention {i}: {attn.shape}\")\n",
    "            return attn_weights[-1]\n",
    "        else:\n",
    "            print(\"No attention weights captured\")\n",
    "            \n",
    "            print(\"Trying direct access to transformer blocks...\")\n",
    "            for name, module in model.named_modules():\n",
    "                if hasattr(module, 'blocks'):\n",
    "                    print(f\"Found transformer blocks in: {name}\")\n",
    "                    try:\n",
    "                        if hasattr(module, 'forward_encoder'):\n",
    "                            result = module.forward_encoder(samples, return_attn=True)\n",
    "                            if isinstance(result, tuple) and len(result) > 1:\n",
    "                                return result[1]\n",
    "                    except:\n",
    "                        pass\n",
    "            \n",
    "            return None\n",
    "\n",
    "def visualize_attention_on_image(image, attn, patch_size=16, head=0, cls_token=True, channel=0):\n",
    "    if isinstance(image, torch.Tensor):\n",
    "        image = image.detach().cpu().numpy()\n",
    "    \n",
    "    if len(image.shape) == 4:\n",
    "        image = image[0]\n",
    "    \n",
    "    if len(image.shape) == 3:\n",
    "        if image.shape[0] >= 3:\n",
    "            rgb_channels = [0, 5, min(10, image.shape[0]-1)]\n",
    "            display_image = np.zeros((image.shape[1], image.shape[2], 3))\n",
    "            for i, ch in enumerate(rgb_channels):\n",
    "                channel_data = image[ch]\n",
    "                p2, p98 = np.percentile(channel_data, [2, 98])\n",
    "                if p98 > p2:\n",
    "                    display_image[:, :, i] = np.clip((channel_data - p2) / (p98 - p2), 0, 1)\n",
    "        else:\n",
    "            display_image = image[channel]\n",
    "            p2, p98 = np.percentile(display_image, [2, 98])\n",
    "            if p98 > p2:\n",
    "                display_image = np.clip((display_image - p2) / (p98 - p2), 0, 1)\n",
    "            display_image = np.stack([display_image] * 3, axis=-1)\n",
    "    else:\n",
    "        display_image = image\n",
    "    \n",
    "    if isinstance(attn, torch.Tensor):\n",
    "        attn = attn.detach().cpu().numpy()\n",
    "    \n",
    "    attn_head = attn[head]\n",
    "    \n",
    "    if cls_token:\n",
    "        attn_map = attn_head[0, 1:]\n",
    "    else:\n",
    "        attn_map = attn_head.mean(axis=0)[1:]\n",
    "    \n",
    "    num_patches = attn_map.shape[0]\n",
    "    grid_size = int(np.sqrt(num_patches))\n",
    "    \n",
    "    if grid_size * grid_size != num_patches:\n",
    "        print(f\"Warning: Cannot reshape {num_patches} patches into square grid\")\n",
    "        grid_size = int(np.sqrt(num_patches))\n",
    "        attn_map = attn_map[:grid_size*grid_size]\n",
    "    \n",
    "    attn_map = attn_map.reshape(grid_size, grid_size)\n",
    "    \n",
    "    if attn_map.max() > attn_map.min():\n",
    "        attn_map = (attn_map - attn_map.min()) / (attn_map.max() - attn_map.min())\n",
    "    \n",
    "    attn_map = np.kron(attn_map, np.ones((patch_size, patch_size)))\n",
    "    \n",
    "    img_h, img_w = display_image.shape[:2]\n",
    "    if attn_map.shape[0] != img_h or attn_map.shape[1] != img_w:\n",
    "        attn_map = ndimage.zoom(attn_map, (img_h/attn_map.shape[0], img_w/attn_map.shape[1]))\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    axes[0].imshow(display_image)\n",
    "    axes[0].set_title('Original Satellite Image')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    im1 = axes[1].imshow(attn_map, cmap='jet')\n",
    "    axes[1].set_title(f'Attention Map (Head {head})')\n",
    "    axes[1].axis('off')\n",
    "    plt.colorbar(im1, ax=axes[1])\n",
    "    \n",
    "    axes[2].imshow(display_image)\n",
    "    axes[2].imshow(attn_map, cmap='jet', alpha=0.4)\n",
    "    axes[2].set_title('Attention Overlay')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return attn_map\n",
    "\n",
    "def visualize_multiple_heads(image, attn, patch_size=16, num_heads=8):\n",
    "    if isinstance(attn, torch.Tensor):\n",
    "        attn = attn.detach().cpu().numpy()\n",
    "    \n",
    "    num_heads = min(num_heads, attn.shape[0])\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for head in range(num_heads):\n",
    "        attn_head = attn[head]\n",
    "        attn_map = attn_head[0, 1:]\n",
    "        \n",
    "        num_patches = attn_map.shape[0]\n",
    "        grid_size = int(np.sqrt(num_patches))\n",
    "        if grid_size * grid_size != num_patches:\n",
    "            grid_size = int(np.sqrt(num_patches))\n",
    "            attn_map = attn_map[:grid_size*grid_size]\n",
    "        \n",
    "        attn_map = attn_map.reshape(grid_size, grid_size)\n",
    "        \n",
    "        if attn_map.max() > attn_map.min():\n",
    "            attn_map = (attn_map - attn_map.min()) / (attn_map.max() - attn_map.min())\n",
    "        \n",
    "        if head < len(axes):\n",
    "            im = axes[head].imshow(attn_map, cmap='jet')\n",
    "            axes[head].set_title(f'Head {head}')\n",
    "            axes[head].axis('off')\n",
    "            plt.colorbar(im, ax=axes[head])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def extract_and_visualize_attention(model, imgs, timestamps, sample_idx=0, timestep=0):\n",
    "    print(\"Extracting attention weights...\")\n",
    "    print(f\"Input shape: {imgs.shape}\")\n",
    "    print(f\"Timestamps shape: {timestamps.shape}\")\n",
    "    \n",
    "    attn_weights = get_last_self_attention(model, imgs, timestamps)\n",
    "    \n",
    "    if attn_weights is not None:\n",
    "        print(f\"Attention weights extracted: {attn_weights.shape}\")\n",
    "        \n",
    "        viz_image = imgs[sample_idx, timestep]\n",
    "        print(f\"Visualization image shape: {viz_image.shape}\")\n",
    "        \n",
    "        print(\"Creating attention visualization...\")\n",
    "        attn_map = visualize_attention_on_image(\n",
    "            viz_image, \n",
    "            attn_weights[sample_idx], \n",
    "            patch_size=16, \n",
    "            head=0\n",
    "        )\n",
    "        \n",
    "        if attn_weights.shape[1] > 1:\n",
    "            print(\"Visualizing multiple attention heads...\")\n",
    "            visualize_multiple_heads(\n",
    "                viz_image, \n",
    "                attn_weights[sample_idx], \n",
    "                patch_size=16, \n",
    "                num_heads=min(8, attn_weights.shape[1])\n",
    "            )\n",
    "        \n",
    "        return attn_weights\n",
    "    else:\n",
    "        print(\"Could not extract attention weights from model\")\n",
    "        return None\n",
    "\n",
    "# STEP 5: RUN ATTENTION EXTRACTION\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"STEP 5: RUNNING ATTENTION EXTRACTION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"Checking variables...\")\n",
    "variables_ok = True\n",
    "\n",
    "if 'model' not in locals():\n",
    "    print(\"model not found\")\n",
    "    variables_ok = False\n",
    "else:\n",
    "    print(f\"model: {type(model)}\")\n",
    "\n",
    "if 'imgs' not in locals():\n",
    "    print(\"imgs not found\") \n",
    "    variables_ok = False\n",
    "elif imgs is None:\n",
    "    print(\"imgs is None\")\n",
    "    variables_ok = False\n",
    "else:\n",
    "    print(f\"imgs: {imgs.shape}\")\n",
    "\n",
    "if 'timestamps' not in locals():\n",
    "    print(\"timestamps not found\")\n",
    "    variables_ok = False\n",
    "elif timestamps is None:\n",
    "    print(\"timestamps is None\")\n",
    "    variables_ok = False\n",
    "else:\n",
    "    print(f\"timestamps: {timestamps.shape}\")\n",
    "\n",
    "if 'pred_imgs' not in locals():\n",
    "    print(\"pred_imgs not found (but we can work without it)\")\n",
    "elif pred_imgs is None:\n",
    "    print(\"pred_imgs is None (model inference failed, but we can still extract attention)\")\n",
    "else:\n",
    "    print(f\"pred_imgs: {pred_imgs.shape}\")\n",
    "\n",
    "if variables_ok:\n",
    "    print(\"\\nRUNNING ATTENTION EXTRACTION...\")\n",
    "    attention_weights = extract_and_visualize_attention(\n",
    "        model, \n",
    "        imgs, \n",
    "        timestamps, \n",
    "        sample_idx=0, \n",
    "        timestep=0\n",
    "    )\n",
    "    \n",
    "    if attention_weights is not None:\n",
    "        print(\"\\nSUCCESS! Attention extraction completed.\")\n",
    "        print(f\"Final attention shape: {attention_weights.shape}\")\n",
    "    else:\n",
    "        print(\"\\nAttention extraction failed.\")\n",
    "else:\n",
    "    print(\"Missing required variables - check the steps above for errors\")\n",
    "    print(\"\\nDebugging info:\")\n",
    "    print(f\"  model exists: {'model' in locals()}\")\n",
    "    print(f\"  imgs exists: {'imgs' in locals()}\")\n",
    "    print(f\"  imgs is not None: {imgs is not None if 'imgs' in locals() else 'N/A'}\")\n",
    "    print(f\"  timestamps exists: {'timestamps' in locals()}\")\n",
    "    print(f\"  timestamps is not None: {timestamps is not None if 'timestamps' in locals() else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c325abf2-221a-479c-9747-cdbc3db937a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:soehrle-satvis_kernel]",
   "language": "python",
   "name": "conda-env-soehrle-satvis_kernel-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
